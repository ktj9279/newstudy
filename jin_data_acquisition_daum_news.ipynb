{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "# from selenium import webdriver\n",
    "import sqlite3\n",
    "import time as time_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "press_list = ['daum', 'naver',\n",
    "              'seoulilbo', 'dtoday', 'asiailbo', 'labortoday', 'm-i',\n",
    "              'ekn', 'busan', 'imaeil', 'kookje', 'yeongnam']\n",
    "press_dict = {'daum':'다음', 'naver':'네이버',\n",
    "              'seoulilbo':'서울일보', 'dtoday':'일간투데이', 'asiailbo':'아시아일보',\n",
    "              'labortoday':'매일노동뉴스', 'm-i':'매일일보', 'ekn':'에너지경제',\n",
    "              'busan':'부산일보', 'imaeil':'매일신문', 'kookje':'국제신문',\n",
    "              'yeongnam':'영남일보'}\n",
    "\n",
    "daum_press_list = ['EBS', 'IT동아', 'JTBC', 'KBS', 'KTV',\n",
    "                   'MBC', 'MBN', 'SBS', 'SBS CNBC', 'YTN',\n",
    "                   'ZDNet Korea', 'bnt뉴스', '게임동아', '게임톡', '경향신문',\n",
    "                   '국민일보', '기자협회보', '노컷뉴스', '뉴스1', '뉴시스',\n",
    "                   '데일리안', '동아사이언스', '동아일보', '디지털타임스', '로이터',\n",
    "                   '매경게임진', '매일경제', '머니S', '머니투데이', '문화일보',\n",
    "                   '미디어오늘', '서울경제', '서울신문', '세계일보', '아시아경제',\n",
    "                   '아이뉴스24', '연합뉴스', '연합뉴스TV', '오마이뉴스', '오토타임즈',\n",
    "                   '이데일리', '전자신문', '조선비즈', '조선일보', '중앙일보',\n",
    "                   '채널A', '코리아헤럴드', '쿠키뉴스', '파이낸셜뉴스', '포토친구',\n",
    "                   '프레시안', '한겨레', '한국경제', '한국경제TV', '한국일보',\n",
    "                   '헤럴드경제']\n",
    "naver_press_list = ['JTBC', 'KBS', 'MBC', 'MBN', 'SBS',\n",
    "                    'SBS CNBC', 'TV조선', 'YTN', 'ZDNet Korea', '강원일보',\n",
    "                    '경향신문', '국민일보', '기자협회보', '노컷뉴스', '뉴스1',\n",
    "                    '뉴시스', '데일리안', '동아사이언스', '동아일보', '디지털데일리',\n",
    "                    '디지털타임스', '로이터', '매일경제', '매일신문', '머니S',\n",
    "                    '머니투데이', '문화일보', '미디어오늘', '부산일보', '블로터',\n",
    "                    '서울경제', '서울신문', '세계일보', '아시아경제', '아이뉴스24',\n",
    "                    '여성신문', '연합뉴스', '연합뉴스TV', '오마이뉴스', '이데일리',\n",
    "                    '일다', '전자신문', '조선비즈', '조선일보', '조세일보',\n",
    "                    '중앙일보', '참세상', '채널A', '코리아헤럴드', '코메디닷컴',\n",
    "                    '파이낸셜뉴스', '프레시안', '한겨레', '한국경제', '한국경제TV',\n",
    "                    '한국일보', '헤럴드경제', '헬스조선']\n",
    "\n",
    "section_list = ['society', 'politics', 'economic', 'culture', 'digital', 'global']\n",
    "section_dict = {'society':'사회', 'politics':'정치', 'economic':'경제',\n",
    "               'culture':'문화', 'digital':'IT', 'global':'세계'}\n",
    "\n",
    "base_urls = {'daum':\n",
    "             {'society':'http://media.daum.net/breakingnews/society',\n",
    "              'politics':'http://media.daum.net/breakingnews/politics',\n",
    "              'economic':'http://media.daum.net/breakingnews/economic',\n",
    "              'culture':'http://media.daum.net/breakingnews/culture',\n",
    "              'digital':'http://media.daum.net/breakingnews/digital',\n",
    "              'global':'http://media.daum.net/breakingnews/foreign'\n",
    "             },\n",
    "             'seoulilbo':\n",
    "             {'society':'http://www.seoulilbo.com/news/articleList.html?sc_section_code=S1N10&view_type=sm',\n",
    "              'politics':'http://www.seoulilbo.com/news/articleList.html?sc_section_code=S1N8&view_type=sm',\n",
    "              'economic':'http://www.seoulilbo.com/news/articleList.html?sc_section_code=S1N9&view_type=sm',\n",
    "              'culture':'http://www.seoulilbo.com/news/articleList.html?sc_section_code=S1N11&view_type=sm',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'dtoday':\n",
    "             {'society':'',\n",
    "              'politics':'http://www.dtoday.co.kr/news/articleList.html?sc_section_code=S1N1&view_type=sm',\n",
    "              'economic':'http://www.dtoday.co.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm',\n",
    "              'culture':'',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'asiailbo':\n",
    "             {'society':'http://www.asiailbo.co.kr/etnews/?cid=21030000',\n",
    "              'politics':'http://www.asiailbo.co.kr/etnews/?cid=21010000',\n",
    "              'economic':'http://www.asiailbo.co.kr/etnews/?cid=21020000',\n",
    "              'culture':'http://www.asiailbo.co.kr/etnews/?cid=21040000',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'labortoday':\n",
    "             {'society':'http://www.labortoday.co.kr/news/articleList.html?sc_section_code=S1N3&view_type=sm',\n",
    "              'politics':'http://www.labortoday.co.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm',\n",
    "              'economic':'',    # 정치, 경제\n",
    "              'culture':'',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'm-i':\n",
    "             {'society':'http://www.m-i.kr/news/articleList.html?sc_section_code=S1N3&view_type=sm',\n",
    "              'politics':'http://www.m-i.kr/news/articleList.html?sc_section_code=S1N1&view_type=tm',\n",
    "              'economic':'http://www.m-i.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm',\n",
    "              'culture':'http://www.m-i.kr/news/articleList.html?sc_section_code=S1N22&view_type=tm',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'ekn':\n",
    "             {'society':'http://www.ekn.kr/news/section_list_all.html?sec_no=25',\n",
    "              'politics':'',    # 정치, 사회\n",
    "              'economic':'http://www.ekn.kr/news/section_list_all.html?sec_no=130',\n",
    "              'culture':'',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'busan':\n",
    "             {'society':'http://news20.busan.com/news/social.jsp',\n",
    "              'politics':'http://news20.busan.com/news/politics.jsp',\n",
    "              'economic':'http://news20.busan.com/EconomyAndOcean/econocean.jsp',\n",
    "              'culture':'http://news20.busan.com/news/culture.jsp',\n",
    "              'digital':'',\n",
    "              'global':''\n",
    "             },\n",
    "             'imaeil':\n",
    "             {'society':'http://news.imaeil.com/SocietyAll/',\n",
    "              'politics':'http://news.imaeil.com/PoliticsAll/',\n",
    "              'economic':'http://news.imaeil.com/EconomyAll/',\n",
    "              'culture':'http://news.imaeil.com/CultureAll/',\n",
    "              'digital':'',\n",
    "              'global':'http://news.imaeil.com/InternationalAll/'\n",
    "             },\n",
    "             'kookje':\n",
    "             {'society':'http://www.kookje.co.kr/sub.htm?code=0300&vHeadTitle=%BB%E7%C8%B8',\n",
    "              'politics':'http://www.kookje.co.kr/sub.htm?code=0100&vHeadTitle=%C1%A4%C4%A1',\n",
    "              'economic':'http://www.kookje.co.kr/sub.htm?code=0200&vHeadTitle=%B0%E6%C1%A6',\n",
    "              'culture':'http://www.kookje.co.kr/sub.htm?code=0500&vHeadTitle=%B9%AE%C8%AD',\n",
    "              'digital':'http://www.kookje.co.kr/sub.htm?code=0800&vHeadTitle=IT%B0%FA%C7%D0',\n",
    "              'global':'http://www.kookje.co.kr/sub.htm?code=0400&vHeadTitle=%B1%B9%C1%A6'\n",
    "             },\n",
    "             'yeongnam':\n",
    "             {'society':'http://www.yeongnam.com/mnews/newsview.do?mode=subMain&cId=04',\n",
    "              'politics':'http://www.yeongnam.com/mnews/newsview.do?mode=subMain&cId=02',\n",
    "              'economic':'http://www.yeongnam.com/mnews/newsview.do?mode=subMain&cId=03',\n",
    "              'culture':'http://www.yeongnam.com/mnews/newsview.do?mode=subMain&cId=08',\n",
    "              'digital':'',    # 교육, 과학\n",
    "              'global':'http://www.yeongnam.com/mnews/newsview.do?mode=subMain&cId=06'\n",
    "             }\n",
    "            }\n",
    "\n",
    "headers = {\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 정규식\n",
    "reg_ex = {'email': r'[a-zA-Z0-9.!#$%&\\'*+/=?^_`{|}~-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',\n",
    "          'sb': r'\\[[^\\]]*\\]',\n",
    "          'pb': r'\\<[^\\>]*\\>'}\n",
    "\n",
    "# 매칭되면 기사를 삭제하는 태그, 단어 및 정규식\n",
    "stop_title_sb = ['[포토]', '[프로필]', '[★화보]', '[게시판]', '[경향포토]',\n",
    "                 '[리빙포인트]', '[머니S포토]', '[부고]', '[부음]', '[사람 人 사람]',\n",
    "                 '[사진]', '[서울포토]', '[영어]', \"[오래전 '이날']\", '[오마이포토]',\n",
    "                 '[이 시각 코스피]', '[이 시각 코스닥]', '[인사]', '[코스닥 공시]', '[코스닥(개장)]',\n",
    "                 '[코스닥(마감)]', '[코스피(개장)]', '[코스피(마감)]', '[포토뉴스]', '[표]',\n",
    "                 '[한경로보뉴스]', '[영상]', '[오늘의 국회 ', '[오늘의 주요일정]']\n",
    "\n",
    "stop_title_pb = ['<부고>', '<오늘의 조간 정치뉴스>', '<인사>', '<포토>', '<코>', '<유>']\n",
    "\n",
    "stop_content_reg_ex = [r'^동영상[\\s]*뉴스']\n",
    "\n",
    "# 매칭되면 해당 내용만 삭제하는 태그, 단어 및 정규식\n",
    "stop_content_reg_ex_crop = [reg_ex['email']]\n",
    "\n",
    "stop_content_reg_ex_ml_crop = [r'^[\\s]*\\【.*?\\】([^=\\n]*기자[\\s]*=)?',\n",
    "                               r'^[\\s]*\\[.*?\\]([^=\\n)]*기자[\\s]*=)?',\n",
    "                               r'^[\\s]*\\(.*?\\)([^=\\n]*기자[\\s]*=)?',\n",
    "                               r'^[ㄱ-ㅎㅏ-ㅣ가-힣]+[\\s]?기자[\\s]?',\n",
    "                               r'^© News1.*\\n',\n",
    "                               '(- Copyrights )?ⓒ[^\\.]*$',\n",
    "                               r'\\【.*?\\】[\\s]*$',\n",
    "                               r'\\[.*?\\][\\s]*$',\n",
    "                               r'\\(.*?\\)[\\s]*$',\n",
    "                               r'[\\s\\.][ㄱ-ㅎㅏ-ㅣ가-힣]+[\\s]?기자$']\n",
    "\n",
    "# stop_content_reg_ex_ml_crop = [r'^[\\s]*\\【[^\\】]*\\】([^=\\n]*기자[\\s]*=)?',\n",
    "#                                r'^[\\s]*\\[[^\\]]*\\]([^=\\n)]*기자[\\s]*=)?',\n",
    "#                                r'^[\\s]*\\([^\\)]*\\)([^=\\n]*기자[\\s]*=)?',\n",
    "#                                r'^[ㄱ-ㅎㅏ-ㅣ가-힣]+[\\s]?기자[\\s]?',\n",
    "#                                r'^© News1.*\\n',\n",
    "#                                '(- Copyrights )?ⓒ[^\\.]*$',\n",
    "#                                r'\\【[^\\【]*\\】[\\s]*$',\n",
    "#                                r'\\[[^\\[]*\\][\\s]*$',\n",
    "#                                r'\\([^\\(]*\\)[\\s]*$',\n",
    "#                                r'[\\s\\.][ㄱ-ㅎㅏ-ㅣ가-힣]+[\\s]?기자$']\n",
    "# r'^[\\s]*\\【[^\\】]*=[^\\】]*\\】([^=\\n]*기자[\\s]*=)?'\n",
    "# r'^[\\s]*\\[[^\\]]*=[^\\]]*\\]([^=\\n)]*기자[\\s]*=)?'\n",
    "# r'^[\\s]*\\([^\\)]*=[^\\)]*\\)([^=\\n]*기자[\\s]*=)?'\n",
    "# /.+\\n\n",
    "# r'^[]{0}[]$'.format(press_ko)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title_delete_article(title):\n",
    "    # Square brackets tag로 시작할 경우\n",
    "    p = r'^' + reg_ex['sb']\n",
    "    m = re.match(p, title)\n",
    "    # 매칭 안 되면 다음 정규식 검사로 넘어감.\n",
    "    if m == None:\n",
    "        pass\n",
    "    # 매칭된 문자열이 불용어 목록에 존재하면 True 반환하여 해당 기사 삭제\n",
    "    elif m[0] in stop_title_sb:\n",
    "        return True\n",
    "    \n",
    "    # Pointy brackets tag로 시작할 경우\n",
    "    p = r'^' + reg_ex['pb']\n",
    "    m = re.match(p, title)\n",
    "    if m == None:\n",
    "#         # [Debug]\n",
    "#         print('None')\n",
    "        pass\n",
    "    elif m[0] in stop_title_pb:\n",
    "#         # [Debug]\n",
    "#         print('Delete')\n",
    "#         print(re.sub(p, '', title))\n",
    "        return True\n",
    "    \n",
    "    # 매칭 되지 않거나 매칭된 문자열이 불용어 목록에 존재하지 않으면 False 반환하여 해당 기사 수집\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Debug]\n",
    "# title = '<부고> 매칭 & 삭제 시 삭제 안 된 나머지 + True, 매칭은 됐으나 삭제 안 될 시 False, 매칭 안 될 시 None + False 출력!'\n",
    "# check_title_delete_article(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_delete_article(content):\n",
    "    # 특정 단어로 시작할 경우\n",
    "    for p in stop_content_reg_ex:\n",
    "        m = re.match(p, content)\n",
    "        # 매칭 안 되면 다음 정규식 검사로 넘어감.\n",
    "        if m == None:\n",
    "#             # [Debug]\n",
    "#             print('None')\n",
    "            pass\n",
    "        # 매칭될 경우 True 반환하여 해당 기사 삭제\n",
    "        else:\n",
    "#             # [Debug]\n",
    "#             print('Delete')\n",
    "#             print(re.sub(p, '', content))\n",
    "            return True\n",
    "    \n",
    "    # 매칭 되지 않거나 매칭된 문자열이 불용어 목록에 존재하지 않으면 False 반환하여 해당 기사 수집\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Debug]\n",
    "# content = \"\"\"동영상 뉴스\n",
    "\n",
    "# 매칭 & 삭제 시 삭제 안 된 나머지 + True, 매칭은 됐으나 삭제 안 될 시 False, 매칭 안 될 시 None + False 출력!\n",
    "# \"\"\"\n",
    "# check_content_delete_article(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_crop_content(content):\n",
    "    # Normal mode\n",
    "    for p in stop_content_reg_ex_crop:\n",
    "        # 매칭될 경우 매칭된 문자열 잘래낸 후 앞뒤 공백 제거\n",
    "        content = re.sub(p, '', content).strip()\n",
    "    \n",
    "    # Multiline mode\n",
    "    for p in stop_content_reg_ex_ml_crop:\n",
    "        content = re.sub(p, '', content, flags=re.MULTILINE).strip()\n",
    "\n",
    "    # 잘라낸 content 반환\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # [Debug]\n",
    "# content = \"\"\"[연합뉴스] 김하나 기자\n",
    "\n",
    "# (연합뉴스) Multiline & non-greedy test :)\n",
    "\n",
    "# 매칭 후 삭제 안 된 나머지만 출력!\n",
    "# \"\"\"\n",
    "# content = check_content_crop_content(content)\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ktj@big.com\n",
      "본문111\n",
      "본문222\n",
      "본문333\n",
      "\n",
      "ktj@big.co.kr\n",
      "\n",
      "본문444\n",
      "본문555\n",
      "본문666\n",
      "\n",
      "ktj@bigdata.com\n",
      "\n",
      "본문777\n",
      "본문888\n",
      "본문999\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "content = \"\"\"ktj@big.com\n",
    "본문111\n",
    "본문222\n",
    "본문333\n",
    "\n",
    "ktj@big.co.kr\n",
    "\n",
    "본문444\n",
    "본문555\n",
    "본문666\n",
    "\n",
    "ktj@bigdata.com\n",
    "\n",
    "본문777\n",
    "본문888\n",
    "본문999\n",
    "\n",
    "ktj@bigdata.co.kr\n",
    "\n",
    "광고111\n",
    "광고222\n",
    "광고333\n",
    "\"\"\"\n",
    "# content 중 가장 마지막 기자 이메일부터 이하 내용(광고 등) 제거\n",
    "try:\n",
    "    p = re.findall(r'.{20}' + reg_ex['email'], content, flags=re.DOTALL)[-1]\n",
    "    email_idx = re.search(p, content).start() + 20\n",
    "    content_temp = content[:email_idx].strip()\n",
    "    # 이메일이 본문 앞이나 중간에 들어가지 않은 경우에만 잘린 본문 사용\n",
    "    if (len(content_temp) / len(content)) > 0.6:\n",
    "        content = content_temp\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE daum(a_ids TEXT primary key, \\\n",
    "                dates DATE, times TIME, titles TEXT, contents TEXT, \\\n",
    "                press TEXT, authors TEXT, sections TEXT, urls TEXT)\")\n",
    "\n",
    "    conn.commit()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap URLs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(date_end_time_delta, day_period):\n",
    "    date_end = datetime.date.today() - datetime.timedelta(date_end_time_delta)\n",
    "    date_start = date_end - datetime.timedelta(day_period - 1)\n",
    "    \n",
    "    return  date_start, date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 첫 번째 인자 기준날짜(str)로 변경 \n",
    "def scrap_daum_news_urls(date_end_time_delta=1, day_period=7, page_limit=10000):\n",
    "    date_start, date_end = get_period(date_end_time_delta, day_period)\n",
    "    \n",
    "    press = 'daum'\n",
    "\n",
    "    # Section iteration\n",
    "    for section in section_list:\n",
    "        if base_urls[press][section] == '':\n",
    "            continue\n",
    "        else:\n",
    "            print(press, section_dict[section], base_urls[press][section])\n",
    "            print(date_start, ' - ', date_end)\n",
    "            print('--------------------------------------------------\\n')\n",
    "            \n",
    "            # Get section\n",
    "            section_ko = section_dict[section]\n",
    "\n",
    "            # Date iteration\n",
    "            for time_delta in range(day_period):\n",
    "                # Get date\n",
    "                date = str(date_end - datetime.timedelta(time_delta))\n",
    "                print(date)\n",
    "\n",
    "                urls = []\n",
    "                \n",
    "                # Page iteration\n",
    "                page = 1\n",
    "                while(True):\n",
    "                    if (page % 100) == 0:\n",
    "                        print('{0:6,} / {1:6,}'.format(page, page_limit))\n",
    "\n",
    "                    req_url = base_urls[press][section] + '?page=' + str(page) + '&regDate=' + date.replace('-', '')\n",
    "                    while(True):\n",
    "                        try:\n",
    "                            resp = requests.get(req_url, headers=headers, timeout=1)\n",
    "                        except:\n",
    "#                             print('Timeout: retry')\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    html = bs(resp.text, \"lxml\")\n",
    "\n",
    "                    url_html = html.select('#mArticle .tit_thumb > .link_txt')                \n",
    "\n",
    "                    urls_len = len(urls)\n",
    "                    \n",
    "                    # Get url\n",
    "                    for i in url_html:\n",
    "                        urls.append(i.get('href'))\n",
    "                        \n",
    "                    urls_len_diff = len(urls) - urls_len\n",
    "\n",
    "                    if page == page_limit:\n",
    "                        break\n",
    "                    elif urls_len_diff == 0:\n",
    "                        print('#', page, ': end of pages.')\n",
    "                        break\n",
    "                    else:    \n",
    "                        page += 1\n",
    "                \n",
    "                with open('db/urls/daum/' + press + '_' + section + '_' + date.replace('-', '') + '.pkl', 'wb') as f:\n",
    "                    pickle.dump(urls, f)\n",
    "\n",
    "            print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrap_daum_news_urls(1, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debug] Scrap URLs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2670"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "press = 'daum'\n",
    "section = 'politics'\n",
    "date = '20180823'\n",
    "\n",
    "with open('db/urls/daum/' + press + '_' + section + '_' + date + '.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "    \n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://v.media.daum.net/v/20180823235953274',\n",
       " 'http://v.media.daum.net/v/20180823235712261',\n",
       " 'http://v.media.daum.net/v/20180823234956198',\n",
       " 'http://v.media.daum.net/v/20180823234929195',\n",
       " 'http://v.media.daum.net/v/20180823234832186',\n",
       " 'http://v.media.daum.net/v/20180823225456524',\n",
       " 'http://v.media.daum.net/v/20180823224023322',\n",
       " 'http://v.media.daum.net/v/20180823223902308',\n",
       " 'http://v.media.daum.net/v/20180823223627276',\n",
       " 'http://v.media.daum.net/v/20180823223304232']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap News\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_daum_news(date_end_time_delta=1, day_period=7):\n",
    "    date_start, date_end = get_period(date_end_time_delta, day_period)\n",
    "    \n",
    "    press = 'daum'\n",
    "\n",
    "    for section in section_list:\n",
    "        if base_urls[press][section] == '':\n",
    "            continue\n",
    "        else:\n",
    "            print(press, section_dict[section], base_urls[press][section])\n",
    "            print(date_start, ' - ', date_end)\n",
    "            print('--------------------------------------------------\\n')\n",
    "            \n",
    "            # Get section\n",
    "            section_ko = section_dict[section]\n",
    "\n",
    "            # Date iteration\n",
    "            for time_delta in range(day_period):\n",
    "                # Get date\n",
    "                date = str(date_end - datetime.timedelta(time_delta))\n",
    "                print(date)\n",
    "\n",
    "                # Open url\n",
    "                with open('db/urls/daum/' + press + '_' + section + '_' + date.replace('-', '') + '.pkl', 'rb') as f:\n",
    "                    urls = pickle.load(f)\n",
    "                    \n",
    "                for idx, url in enumerate(urls):\n",
    "                    if (idx % 1000) == 0:\n",
    "                        print('{0:6,} / {1:6,}'.format(idx, len(urls)))\n",
    "\n",
    "                    while(True):\n",
    "                        try:\n",
    "                            resp = requests.get(url, headers=headers, timeout=1)\n",
    "#                             time_module.sleep(0.5)\n",
    "                        except:\n",
    "#                             print('Timeout: retry')\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    html = bs(resp.text, \"lxml\")\n",
    "\n",
    "                    # Get id\n",
    "                    a_id = 'da_' + url[-17:]\n",
    "                    \n",
    "                    # Get time\n",
    "                    date_html = html.select('.head_view .info_view')\n",
    "#                         date = re.search(r'\\d{2,4}[.-]?\\d+[.-]?.\\d+', date_html[0].text)[0]\n",
    "                    try:\n",
    "                        time = re.search(r'\\d+:\\d+', date_html[0].text)[0]\n",
    "                    except:\n",
    "                        time = None\n",
    "                \n",
    "                    # Get title\n",
    "                    # title이 빈 문자열이면 해당 기사 제외\n",
    "                    title_html = html.select('.head_view .tit_view')\n",
    "                    try:\n",
    "                        title = title_html[0].text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    # Check title\n",
    "                    # title에 한글이 하나도 없으면 해당 기사 제외\n",
    "                    if re.search(r'[ㄱ-ㅎㅏ-ㅣ가-힣]+', title) == None:\n",
    "                        continue\n",
    "                    # 특정 title의 기사 제외\n",
    "                    if check_title_delete_article(title):\n",
    "                        continue\n",
    "                    \n",
    "                    # Get content\n",
    "                    # content가 빈 문자열이면 해당 기사 제외\n",
    "                    content_html = html.select('.news_view #harmonyContainer')\n",
    "                    try:\n",
    "                        content = content_html[0].text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check content\n",
    "                    # content 중 가장 마지막 기자 이메일부터 이하 내용(광고 등) 제거\n",
    "                    try:\n",
    "                        p = re.findall(r'.{20}' + reg_ex['email'], content, flags=re.DOTALL)[-1]\n",
    "                        email_idx = re.search(p, content).start() + 20\n",
    "                        content_temp = content[:email_idx].strip()\n",
    "                        # 이메일이 본문 앞이나 중간에 들어가지 않은 경우에만 잘린 본문 사용\n",
    "                        if (len(content_temp) / len(content)) > 0.6:\n",
    "                            content = content_temp\n",
    "                    except:\n",
    "                        pass\n",
    "                    # 특정 content의 기사 제외\n",
    "                    if check_content_delete_article(content):\n",
    "                        continue\n",
    "                    # content 중 특정 내용 제거\n",
    "                    content = check_content_crop_content(content)\n",
    "                        \n",
    "                    # TODO: 예외 때문에 다른 방식을 도입하는 것이 좋을 듯하여 제거 예정\n",
    "                    # content 중 ▶ 이하 내용 제거 (naver에 적용했던 코드이므로 수정해서 사용할 것)\n",
    "#                         tempIdx = cont.index('▶')\n",
    "#                         if tempIdx > 0:\n",
    "#                             cont = cont[:tempIdx]\n",
    "\n",
    "                    # Get author\n",
    "                    # TODO: 정규식 수정\n",
    "                    try:\n",
    "                        author = re.search(r'\\w+\\s*기자', content_html[0].text)[0].replace(' 기자', '').replace('기자', '')\n",
    "                    except:\n",
    "                        author = None\n",
    "\n",
    "                    # Get press\n",
    "                    press_html = html.select('.head_view .thumb_g')\n",
    "                    try:\n",
    "                        press_ko = press_html[0].get('alt')\n",
    "                    except:\n",
    "                        press_ko = None\n",
    "\n",
    "                    # Insert data (row by row)\n",
    "                    data = (a_id, date, time, title, content, press_ko, author, section_ko, url)\n",
    "                    try:\n",
    "                        cur.execute(\"INSERT INTO daum(a_ids, dates, times, titles, contents, press, authors, sections, urls) \\\n",
    "                                    values(?,?,?,?,?,?,?,?,?)\", data)\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        conn.commit()\n",
    "\n",
    "            print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daum 사회 http://media.daum.net/breakingnews/society\n",
      "2018-08-23  -  2018-08-23\n",
      "--------------------------------------------------\n",
      "\n",
      "2018-08-23\n",
      "     0 /  7,994\n",
      " 1,000 /  7,994\n",
      " 2,000 /  7,994\n"
     ]
    }
   ],
   "source": [
    "scrap_daum_news(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 에러 발생 시 DB의 특정 section data 삭제\n",
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DELETE FROM daum WHERE sections='IT'\")\n",
    "\n",
    "# conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
