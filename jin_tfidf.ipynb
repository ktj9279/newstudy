{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from math import log2, log10\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list = ['society', 'politics', 'economic', 'culture', 'digital', 'global']\n",
    "section_dict = {'society':'사회', 'politics':'정치', 'economic':'경제',\n",
    "               'culture':'문화', 'digital':'IT', 'global':'세계'}\n",
    "\n",
    "date_list = ['2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17',\n",
    "             '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 변수\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시로 데이터를 보관하는 변수. DB 또는 pickle로 저장함.\n",
    "# 여러 개의 기준이 있을 수 있음. 주석 참조.\n",
    "a_nouns_tf = {}          # Article\n",
    "a_noun_max_cnt = {}      # Article\n",
    "inverted_idx = {}        # 전체, 기간, section + 기간\n",
    "unique_nouns_idf = {}    # 전체, 기간, section + 기간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# try:\n",
    "#     cur.execute(\"CREATE TABLE Term(a_t_id      INTEGER PRIMARY KEY, \\\n",
    "#                                    a_id        TEXT, \\\n",
    "#                                    term        TEXT, \\\n",
    "#                                    tf_article  REAL, \\\n",
    "#                                    tfidf       REAL, \\\n",
    "#                                    date        DATE, \\ \n",
    "#                                    FOREIGN KEY (a_id) REFERENCES Article(a_id))\")\n",
    "#     conn.commit()\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE U_Term(u_term      TEXT PRIMARY KEY, \\\n",
    "                                     idf         REAL)\")\n",
    "    conn.commit()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(\"DROP TABLE Term\")\n",
    "\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거\n",
    "---\n",
    "\n",
    "* by 한홀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NewsStopWord(word):\n",
    "    try:\n",
    "        int(word) #숫자일경우\n",
    "    except:        \n",
    "        if type(word) is str and word.__contains__('회차'):\n",
    "            return True\n",
    "        if len(word) == 1:\n",
    "            # 한글자 빠짐\n",
    "            return True\n",
    "        if re.search(r'\\d+',word) != None:\n",
    "            # 숫자가 하나라도 포함되면\n",
    "            return True\n",
    "               \n",
    "        newsDic = {\"기자\":1,\"배포\":1,\"금지\":1,\"뉴스\":1,\"저작권자\":1,\n",
    "                   \"기사\":1,\"전재\":1,\"무단\":1,\"무단전재\":1,\"구독\":1,\"기사보기\":1}\n",
    "        \n",
    "        pressDic = {'연합뉴스':1,'뉴시스':1,'뉴시스통신사':1,'통신사':1,\n",
    "                    '이데일리':1,'네이버':1,'다음':1,'시스':1,'뉴스1':1,'뉴스1코리':1}\n",
    "        \n",
    "        nothingDic = {'사진':1,'페이스북':1,'관련':1,'웹툰보기':1,'가기':1,'만큼':1,\n",
    "                     '최근':1,'재인':1,'올해':1,'시간':1,'판단':1,'추진':1,'우리':1,'반영':1,\n",
    "                      '상황':1,'호텔':1,'운영':1,'주요':1,'적극':1,'대상':1,'때문':1,\n",
    "                      '확인':1,'가능':1,'이야기':1,'규모':1,'개월':1,'종합':1,'위원회':1,\n",
    "                      '가운데':1,'분석':1,'다양':1,'문제':1,'기간':1,'마련':1,'지난해':1,'신청':1,'한편':1,'기준':1,\n",
    "                      '내용':1,'채널설정':1,'경우':1,'방안':1,'활용':1,'여러분':1,'기존':1,'최대':1,'스냅':1,'오전':1,'대비':1,\n",
    "                      '위원':1,'지난달':1,'이번달':1,'다음달':1,'위원장':1,'센터':1,'포함':1,'등에':1,'사진영상부':1,\n",
    "                      '구성':1,'수준':1,'기대':1,'공동':1,'안내':1,'활동':1,'첫날':1,'추가':1,'분야':1,'관리':1,\n",
    "                      '동안':1,'이용':1,'모습':1,'오늘':1,'논의':1,'입장':1,'업계':1,'내년':1,'블록':1,'체인':1,'실시간':1,\n",
    "                      '고객':1,'채널':1,'보기':1,'오후':1,'이번':1,'이날':1,'진행':1,'제공':1,'예정':1,'연합':1,'대표':1,\n",
    "                      '제보':1,'이상':1,'지원':1,'행사':1,'관계자':1,'설정':1,'계획':1,'단체':1,'타임':1,'이후':1,'발표':1\n",
    "                     }\n",
    "        if word in newsDic.keys() or word in pressDic.keys() or word in nothingDic.keys():\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Nouns and Compute Term Frequency (TF)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(obj):\n",
    "    try:\n",
    "        float(obj)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(content, k_ratio=0.5):\n",
    "    a_nouns_tf= {}\n",
    "    nouns_cnt = {}\n",
    "    a_noun_max_cnt = 0\n",
    "\n",
    "    # 연속된 공백 및 개행 제거\n",
    "    content = re.sub(r'[\\s]{2,}', ' ', content)\n",
    "    content = re.sub(r'[\\n]{2,}', '\\n', content)\n",
    "\n",
    "    # 문장 단위 토큰화\n",
    "    for sentence in nltk.sent_tokenize(content):\n",
    "        # 단어 단위 토큰화\n",
    "        for word in nltk.word_tokenize(sentence.strip()):\n",
    "            # 명사 추출\n",
    "            nouns = k.nouns(word)\n",
    "\n",
    "            for noun in nouns:\n",
    "                # 불용어 제거\n",
    "                if NewsStopWord(noun):\n",
    "                    continue\n",
    "\n",
    "                if noun in nouns_cnt.keys():\n",
    "                    nouns_cnt[noun] += 1\n",
    "                else:\n",
    "                    nouns_cnt[noun] = 1\n",
    "\n",
    "                if a_noun_max_cnt < nouns_cnt[noun]:\n",
    "                    a_noun_max_cnt = nouns_cnt[noun]\n",
    "\n",
    "    for noun in nouns_cnt.keys():\n",
    "        a_nouns_tf[noun] = k_ratio + (1 - k_ratio) * nouns_cnt[noun] / a_noun_max_cnt\n",
    "\n",
    "    return a_nouns_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_tf(a_id, content):\n",
    "#     a_nouns_tf[a_id] = {}\n",
    "#     nouns_cnt = {}\n",
    "#     a_noun_max_cnt[a_id] = 0\n",
    "\n",
    "#     # 연속된 공백 및 개행 제거\n",
    "#     content = re.sub(r'[\\s]{2,}', ' ', content)\n",
    "#     content = re.sub(r'[\\n]{2,}', '\\n', content)\n",
    "\n",
    "#     # 문장 단위 토큰화\n",
    "#     for idx, sentence in enumerate(nltk.sent_tokenize(content)):\n",
    "#         nouns_temp = []\n",
    "\n",
    "#         # 단어 단위 토큰화\n",
    "#         for word in nltk.word_tokenize(sentence.strip()):\n",
    "#             # 명사 추출\n",
    "#             nouns = k.nouns(word)\n",
    "\n",
    "#             for noun in nouns:\n",
    "#                 # 불용어 제거\n",
    "#                 if NewsStopWord(noun):\n",
    "#                     continue\n",
    "\n",
    "#                 if noun in nouns_cnt.keys():\n",
    "#                     nouns_cnt[noun] += 1\n",
    "#                 else:\n",
    "#                     nouns_cnt[noun] = 1\n",
    "\n",
    "#                 if a_noun_max_cnt[a_id] < nouns_cnt[noun]:\n",
    "#                     a_noun_max_cnt[a_id] = nouns_cnt[noun]\n",
    "\n",
    "# #             # 기본 불용어 제거\n",
    "# #             for stop_word in stop_words:\n",
    "# #                 nouns.remove(stop_word)\n",
    "\n",
    "# #             nouns_temp.extend(nouns)\n",
    "# #             nouns_temp = list(set(nouns_temp))\n",
    "\n",
    "# #         unique_nouns.extend(nouns_temp)\n",
    "# #         unique_nouns = list(set(unique_nouns))\n",
    "\n",
    "#     for noun in nouns_cnt.keys():\n",
    "#         a_nouns_tf[a_id][noun] = k_ratio + (1 - k_ratio) * nouns_cnt[noun] / a_noun_max_cnt[a_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debug] Extract Nouns and Compute Term Frequency (TF)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = \"\"\"제19호 태풍 ‘솔릭’이 제주도를 지나 북상하면서 광주 지역 모든 학교 학생들이 조기 하교 했다. 항공편은 모두 결항됐으며 무등산 입산이 통제됐다. \n",
    "\n",
    "#            23일 오후 광주 서구 하늘에 제19호 태풍 솔릭이 몰고온 먹구름이 가득하다.\n",
    "          \n",
    "\n",
    "# 23일 광주지방기상청에 따르면 제19호 태풍 ‘솔릭’은 이날 낮 12시 현재 제주 서귀포 서쪽 90km 부근 해상에서 시속 4km로 북진 하고 있다. 기상청은 태풍이 이날 오후 6시쯤 목포 서남쪽 80㎞ 해상을 지난뒤 오는 24일 새벽 전북 군산 인근으로 상륙할 것으로 보고 있다. \n",
    "# 솔릭이 예상보다 훨씬 느린 속도로 접근하면서 광주 시민들은 ‘조마조마’한 심정으로 피해 예방을 위해 총력을 다했다. 태풍의 영향으로 광주 지역에는 바람이 점차 강해지면서 이날 제주와 김포를 오가는 광주공항의 모든 항공편이 결항됐다. 무등산도 입산이 통제됐다. \n",
    "# 태풍이 접근하면서 광주시교육청은 전체 학교를 대상으로 ‘조기 하교’를 결정했다. 광주지역 유치원과 초·중·고는 오후 3시 이전에 조기 하교했다. 고등학교의 아간 자율학습도 금지됐다. 교육청은 학원에도 휴원을 적극 검토하고 하원 시간을 조정하도록 요청했다. \n",
    "# 광주·전남은 24일까지 100∼250㎜의 비가 내리고 해안과 지리산에는 400㎜ 넘게 내리는 곳도 있겠다. 광주·전남은 태풍의 중심에서 반경 25m 범위 안에 들어 바람도 강하게 불 것으로 기상청은 내다보고 있다.\"\"\"\n",
    "\n",
    "# a_nouns_tf = compute_tf(content)\n",
    "\n",
    "# a_nouns_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Nouns and TFs into Term Table\n",
    "---\n",
    "\n",
    "* DB에는 2018-08-23만 기록 (2018-08-17 - 2018-08-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-24\n",
      "--------------------------------------------------\n",
      "\n",
      "     0 / 10,530 |    12,830,024\n",
      " 5,000 / 10,530 |    13,266,291\n",
      "10,000 / 10,530 |    13,746,921\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "2018-08-25\n",
      "--------------------------------------------------\n",
      "\n",
      "     0 /  3,694 |    13,789,568\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "2018-08-26\n",
      "--------------------------------------------------\n",
      "\n",
      "     0 /  5,859 |    14,124,465\n",
      " 5,000 /  5,859 |    14,634,001\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "2018-08-27\n",
      "--------------------------------------------------\n",
      "\n",
      "     0 / 12,729 |    14,689,000\n",
      " 5,000 / 12,729 |    15,142,085\n",
      "10,000 / 12,729 |    15,633,897\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date iteration (날짜별로 나눠서 처리)\n",
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24',\n",
    "             '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17',\n",
    "             '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']\n",
    "for date in date_list[:4]:\n",
    "# for date in date_list:\n",
    "    print(date)\n",
    "    print('--------------------------------------------------\\n')\n",
    "    \n",
    "    cur.execute(\"SELECT a_id, content FROM Article WHERE date = '{0}'\".format(date))\n",
    "    data = cur.fetchall()\n",
    "\n",
    "    # Content iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 5000) == 0:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM Term\")\n",
    "            t_size = cur.fetchone()[0]\n",
    "            \n",
    "            print('{0:6,} / {1:6,} | {2:13,}'.format(data_idx, len(data), t_size))\n",
    "            \n",
    "        # TF 계산\n",
    "        a_nouns_tf = compute_tf(content=d[1])\n",
    "        \n",
    "        # Insert data\n",
    "        for noun, tf in a_nouns_tf.items():\n",
    "            record = (d[0], noun, tf, date)\n",
    "            try:\n",
    "                cur.execute(\"INSERT INTO Term(a_id, term, tf_article, date) \\\n",
    "                            VALUES(?,?,?,?)\", record)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            conn.commit()\n",
    "\n",
    "    print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15840782,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"SELECT COUNT(*) FROM Term\")\n",
    "# t_size = cur.fetchone()\n",
    "\n",
    "# t_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 에러 발생 또는 중간에 끊을 시 DB의 특정 데이터 삭제\n",
    "# date = '2018-08-27'\n",
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DELETE FROM Term \\\n",
    "#             WHERE EXISTS (SELECT a_id \\\n",
    "#                           FROM Article \\\n",
    "#                           WHERE Article.date = '{0}' AND Article.a_id = Term.a_id )\".format(date))\n",
    "\n",
    "# conn.commit()\n",
    "\n",
    "# for date in date_list[]:\n",
    "#     conn = sqlite3.connect('db/news_db.db')\n",
    "#     cur = conn.cursor()\n",
    "\n",
    "#     cur.execute(\"DELETE FROM Term \\\n",
    "#                 WHERE EXISTS (SELECT a_id \\\n",
    "#                               FROM Article \\\n",
    "#                               WHERE Article.date = '{0}' AND Article.a_id = Term.a_id )\".format(date))\n",
    "\n",
    "#     conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "너무 오래걸려서 다른 컴퓨터로 돌려서 pickle로 저장해놨다가 합침."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date iteration (날짜별로 나눠서 처리)\n",
    "date_list = ['2018-08-27']\n",
    "for date in date_list:\n",
    "# for date in date_list[-4:-3]:\n",
    "    print(date)\n",
    "    print('--------------------------------------------------\\n')\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    cur.execute(\"SELECT a_id, content FROM Article WHERE date = '{0}'\".format(date))\n",
    "    data = cur.fetchall()\n",
    "\n",
    "    # Content iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 5000) == 0:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM Term\")\n",
    "            t_size = cur.fetchone()[0]\n",
    "            \n",
    "            print('{0:6,} / {1:6,} | {2:13,}'.format(data_idx, len(data), t_size))\n",
    "            \n",
    "        # TF 계산\n",
    "        a_nouns_tf = compute_tf(content=d[1])\n",
    "        \n",
    "        # Insert data\n",
    "        for noun, tf in a_nouns_tf.items():\n",
    "            record = (d[0], noun, tf, date)\n",
    "            records.append(record)\n",
    "#             try:\n",
    "#                 cur.execute(\"INSERT INTO Term(a_id, term, tf_article) \\\n",
    "#                             VALUES(?,?,?)\", record)\n",
    "#             except:\n",
    "#                 pass\n",
    "#         else:\n",
    "#             conn.commit()\n",
    "\n",
    "    with open('temp/records_{0}.pkl'.format(date), 'wb') as f:\n",
    "        pickle.dump(records, f)\n",
    "\n",
    "    print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,256,343\n",
      "\n",
      "('da_20180813235953261', '충남', 0.75)\n",
      "('da_20180813235953261', '천안', 0.8125)\n",
      "('da_20180813235953261', '현금', 1.0)\n",
      "('da_20180813235953261', '수송', 0.875)\n",
      "('da_20180813235953261', '현금수송업체', 0.5625)\n",
      "('da_20180813235953261', '업체', 0.6875)\n",
      "('da_20180813235953261', '직원', 0.625)\n",
      "('da_20180813235953261', '보령', 0.625)\n",
      "('da_20180813235953261', '검거', 0.625)\n",
      "('da_20180813235953261', '천안서북경찰서', 0.6875)\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "date = '2018-08-13'\n",
    "\n",
    "with open('temp/records_{0}.pkl'.format(date), 'rb') as f:\n",
    "    records = pickle.load(f)\n",
    "\n",
    "print('{0:,}\\n'.format(len(records)))\n",
    "    \n",
    "for d in records[:10]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Insert pickled data into Term table\n",
    "# for date in date_list[-4:]:    # ['2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']\n",
    "#     with open('temp/records_{0}.pkl'.format(date), 'rb') as f:\n",
    "#         records = pickle.load(f)\n",
    "\n",
    "#     print('{0} | {1}'.format(date, len(records))\n",
    "#     print('--------------------------------------------------\\n')\n",
    "        \n",
    "#     for record in records:\n",
    "#         try:\n",
    "#             cur.execute(\"INSERT INTO Term(a_id, term, tf_article) \\\n",
    "#                         VALUES(?,?,?)\", record)\n",
    "#         except:\n",
    "#             pass\n",
    "#     else:\n",
    "#         conn.commit()\n",
    "            \n",
    "#     print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Unique Nouns, Invert Index and\n",
    "## Compute Inverse Document Frequency (IDF)\n",
    "---\n",
    "\n",
    "* Inverted index and IDF (전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "inverted_idx = {}\n",
    "unique_nouns_idf = {}\n",
    "\n",
    "cur.execute(\"SELECT a_id, term FROM Term\")\n",
    "\n",
    "data = cur.fetchall()\n",
    "\n",
    "# Noun iteration\n",
    "for data_idx, d in enumerate(data):\n",
    "    if (data_idx % 1000000) == 0:\n",
    "        print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "    \n",
    "    a_id = d[0]\n",
    "    noun = d[1]\n",
    "    \n",
    "    if noun in inverted_idx.keys():\n",
    "        inverted_idx[noun].append(a_id)\n",
    "    else:\n",
    "        inverted_idx[noun] = []\n",
    "        inverted_idx[noun].append(a_id)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(*) FROM Article\")\n",
    "\n",
    "a_size = cur.fetchone()[0]\n",
    "        \n",
    "for noun, a_ids in inverted_idx.items():\n",
    "    unique_nouns_idf[noun] = log10(a_size / len(a_ids))\n",
    "        \n",
    "with open('db/inverted_index/inverted_index.pkl', 'wb') as f:\n",
    "    pickle.dump(inverted_idx, f)\n",
    "    \n",
    "with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  382153 \n",
      "\n",
      "화재\n",
      "DF:  3866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['da_20180823235953274',\n",
       " 'da_20180823224951448',\n",
       " 'da_20180823224523381',\n",
       " 'da_20180823213907297',\n",
       " 'da_20180823213513245',\n",
       " 'da_20180823212259097',\n",
       " 'da_20180823212032071',\n",
       " 'da_20180823211324987',\n",
       " 'da_20180823205613743',\n",
       " 'da_20180823202638291']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug] Inverted index\n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of IDF unique nouns:  382153 \n",
      "\n",
      "화재 |  1.55\n",
      "화재원인 |  2.68\n",
      "원인 |  1.27\n",
      "어디 |   1.7\n",
      "인천 |  1.43\n",
      "윤태현 |   3.4\n",
      "태현 |  3.01\n",
      "인천시 |  2.13\n",
      "남동 |  2.16\n",
      "남동구 |  2.53\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index and IDF (기간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17']\n",
      "['2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16']\n",
      "['2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15']\n",
      "['2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14']\n",
      "['2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13']\n",
      "['2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12']\n",
      "['2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11']\n",
      "['2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "for date_idx in range(len(date_list)-6):\n",
    "    print(date_list[date_idx:date_idx+7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27\n",
      "14\n",
      "['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24', '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14']\n",
      "2018-08-26\n",
      "14\n",
      "['2018-08-26', '2018-08-25', '2018-08-24', '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13']\n",
      "2018-08-25\n",
      "14\n",
      "['2018-08-25', '2018-08-24', '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12']\n",
      "2018-08-24\n",
      "14\n",
      "['2018-08-24', '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11']\n"
     ]
    }
   ],
   "source": [
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24',\n",
    "             '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17',\n",
    "             '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11']#, '2018-08-10']\n",
    "# [Debug]\n",
    "for date_idx in range(len(date_list)-13):\n",
    "    print(date_list[date_idx])\n",
    "    print(len(date_list[date_idx:date_idx+14]))\n",
    "    print(date_list[date_idx:date_idx+14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 | 12,782,143 |    138,486\n",
      "         0 / 12,782,143\n",
      " 1,000,000 / 12,782,143\n",
      " 2,000,000 / 12,782,143\n",
      " 3,000,000 / 12,782,143\n",
      " 4,000,000 / 12,782,143\n",
      " 5,000,000 / 12,782,143\n",
      " 6,000,000 / 12,782,143\n",
      " 7,000,000 / 12,782,143\n",
      " 8,000,000 / 12,782,143\n",
      " 9,000,000 / 12,782,143\n",
      "10,000,000 / 12,782,143\n",
      "11,000,000 / 12,782,143\n",
      "12,000,000 / 12,782,143\n",
      "2018-08-22 | 12,886,704 |    139,274\n",
      "         0 / 12,886,704\n",
      " 1,000,000 / 12,886,704\n",
      " 2,000,000 / 12,886,704\n",
      " 3,000,000 / 12,886,704\n",
      " 4,000,000 / 12,886,704\n",
      " 5,000,000 / 12,886,704\n",
      " 6,000,000 / 12,886,704\n",
      " 7,000,000 / 12,886,704\n",
      " 8,000,000 / 12,886,704\n",
      " 9,000,000 / 12,886,704\n",
      "10,000,000 / 12,886,704\n",
      "11,000,000 / 12,886,704\n",
      "12,000,000 / 12,886,704\n",
      "2018-08-21 | 12,810,373 |    138,516\n",
      "         0 / 12,810,373\n",
      " 1,000,000 / 12,810,373\n",
      " 2,000,000 / 12,810,373\n",
      " 3,000,000 / 12,810,373\n",
      " 4,000,000 / 12,810,373\n",
      " 5,000,000 / 12,810,373\n",
      " 6,000,000 / 12,810,373\n",
      " 7,000,000 / 12,810,373\n",
      " 8,000,000 / 12,810,373\n",
      " 9,000,000 / 12,810,373\n",
      "10,000,000 / 12,810,373\n",
      "11,000,000 / 12,810,373\n",
      "12,000,000 / 12,810,373\n",
      "2018-08-20 | 12,768,302 |    138,030\n",
      "         0 / 12,768,302\n",
      " 1,000,000 / 12,768,302\n",
      " 2,000,000 / 12,768,302\n",
      " 3,000,000 / 12,768,302\n",
      " 4,000,000 / 12,768,302\n",
      " 5,000,000 / 12,768,302\n",
      " 6,000,000 / 12,768,302\n",
      " 7,000,000 / 12,768,302\n",
      " 8,000,000 / 12,768,302\n",
      " 9,000,000 / 12,768,302\n",
      "10,000,000 / 12,768,302\n",
      "11,000,000 / 12,768,302\n",
      "12,000,000 / 12,768,302\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date (기간) iteration\n",
    "for date_idx in range(len(date_list)-13):\n",
    "# for date_idx in range(len(date_list)-6):\n",
    "    inverted_idx = {}\n",
    "    unique_nouns_idf = {}\n",
    "    \n",
    "    data = []\n",
    "    a_size = 0\n",
    "    \n",
    "    # Date (기간에 속하는 각 날짜) iteration\n",
    "    for date in date_list[date_idx:date_idx+14]:\n",
    "#     for date in date_list[date_idx:date_idx+7]:\n",
    "        cur.execute(\"SELECT T.a_id, T.term \\\n",
    "                    From Article A, Term T \\\n",
    "                    WHERE A.a_id = T.a_id AND A.date = '{0}'\".format(date))\n",
    "\n",
    "        data.extend(cur.fetchall())\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM Article WHERE date = '{0}'\".format(date))\n",
    "\n",
    "        a_size += cur.fetchone()[0]\n",
    "\n",
    "    print('{0} | {1:10,} | {2:10,}'.format(date_list[date_idx], len(data), a_size))\n",
    "        \n",
    "    # Noun iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 5000000) == 0:\n",
    "            print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "        a_id = d[0]\n",
    "        noun = d[1]\n",
    "\n",
    "        if noun in inverted_idx.keys():\n",
    "            inverted_idx[noun].append(a_id)\n",
    "        else:\n",
    "            inverted_idx[noun] = []\n",
    "            inverted_idx[noun].append(a_id)\n",
    "            \n",
    "    for noun, a_ids in inverted_idx.items():\n",
    "        unique_nouns_idf[noun] = log10(a_size / len(a_ids))\n",
    "\n",
    "#     with open('db/inverted_index/inverted_index_' + date_list[date_idx] + '_term.pkl', 'wb') as f:\n",
    "#         pickle.dump(inverted_idx, f)\n",
    "        \n",
    "    with open('db/unique_nouns_idf/unique_nouns_idf_' + date_list[date_idx] + '_term.pkl', 'wb') as f:\n",
    "        pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Debug] Inverted index\n",
    "# date = '2018-08-23'\n",
    "\n",
    "# with open('db/inverted_index/inverted_index_{0}_term.pkl'.format(date), 'rb') as f:\n",
    "#     inverted_idx = pickle.load(f)\n",
    "\n",
    "# unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "# print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "# print(unique_noun)\n",
    "# print('DF: ', len(inverted_idx[unique_noun]))\n",
    "# inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of IDF unique nouns:  380967 \n",
      "\n",
      "김선영 |  3.66\n",
      "선영 |  3.02\n",
      "고교 |  2.06\n",
      "시절 |   1.6\n",
      "대학 |  1.41\n",
      "입시 |  2.34\n",
      "입시만 |  5.14\n",
      "공부 |  2.11\n",
      "의미 |  1.32\n",
      "생각 |  1.07\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "date = '2018-08-27'\n",
    "\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}_term.pkl'.format(date), 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "    \n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index and IDF (section + 기간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24',\n",
    "             '2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17',\n",
    "             '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11']#, '2018-08-10']\n",
    "\n",
    "# Section interation\n",
    "for section in section_list:\n",
    "    # Date (기간) iteration\n",
    "    for date_idx in range(len(date_list)-13):\n",
    "#     for date_idx in range(len(date_list)-6):\n",
    "        inverted_idx = {}\n",
    "        unique_nouns_idf = {}\n",
    "        \n",
    "        data = []\n",
    "        a_size = 0\n",
    "    \n",
    "        # Date (기간에 속하는 각 날짜) iteration\n",
    "        for date in date_list[date_idx:date_idx+14]:\n",
    "    #     for date in date_list[date_idx:date_idx+7]:\n",
    "            cur.execute(\"SELECT T.a_id, T.term \\\n",
    "                        From Article A, Term T \\\n",
    "                        WHERE A.a_id = T.a_id AND A.section = '{0}' AND A.date = '{1}'\".format(section_dict[section], date))\n",
    "\n",
    "            data.extend(cur.fetchall())\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(*) FROM Article WHERE section = '{0}' AND date = '{1}'\".format(section_dict[section], date))\n",
    "\n",
    "            a_size += cur.fetchone()[0]\n",
    "\n",
    "        print('{0} | {1} | {2:10,} | {3:10,}'.format(section, date_list[date_idx], len(data), a_size))\n",
    "\n",
    "        # Noun iteration\n",
    "        for data_idx, d in enumerate(data):\n",
    "#             if (data_idx % 100000) == 0:\n",
    "#                 print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "            a_id = d[0]\n",
    "            noun = d[1]\n",
    "\n",
    "            if noun in inverted_idx.keys():\n",
    "                inverted_idx[noun].append(a_id)\n",
    "            else:\n",
    "                inverted_idx[noun] = []\n",
    "                inverted_idx[noun].append(a_id)\n",
    "                \n",
    "        for noun, a_ids in inverted_idx.items():\n",
    "            unique_nouns_idf[noun] = log10(a_size / len(a_ids))\n",
    "\n",
    "        with open('db/inverted_index/inverted_index_' + section + '_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(inverted_idx, f)\n",
    "            \n",
    "        with open('db/unique_nouns_idf/unique_nouns_idf_' + section + '_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Debug]\n",
    "section = 'society'\n",
    "date = '2018-08-27'\n",
    "\n",
    "with open('db/inverted_index/inverted_index_{0}_{1}_term.pkl'.format(section, date), 'rb') as f:\n",
    "    inverted_idx = pickle.load(f)\n",
    "    \n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "section = 'society'\n",
    "date = '2018-08-27'\n",
    "\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}_{1}_term.pkl'.format(section, date), 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "    \n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index (날짜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 |  1,252,237\n",
      "         0 /  1,252,237\n",
      " 1,000,000 /  1,252,237\n",
      "2018-08-22 |  1,214,585\n",
      "         0 /  1,214,585\n",
      " 1,000,000 /  1,214,585\n",
      "2018-08-21 |  1,232,416\n",
      "         0 /  1,232,416\n",
      " 1,000,000 /  1,232,416\n",
      "2018-08-20 |  1,175,137\n",
      "         0 /  1,175,137\n",
      " 1,000,000 /  1,175,137\n",
      "2018-08-19 |    503,061\n",
      "         0 /    503,061\n",
      "2018-08-18 |    306,049\n",
      "         0 /    306,049\n",
      "2018-08-17 |    997,190\n",
      "         0 /    997,190\n",
      "2018-08-16 |  1,292,530\n",
      "         0 /  1,292,530\n",
      " 1,000,000 /  1,292,530\n",
      "2018-08-15 |    622,358\n",
      "         0 /    622,358\n",
      "2018-08-14 |  1,175,822\n",
      "         0 /  1,175,822\n",
      " 1,000,000 /  1,175,822\n",
      "2018-08-13 |  1,256,343\n",
      "         0 /  1,256,343\n",
      " 1,000,000 /  1,256,343\n",
      "2018-08-12 |    488,204\n",
      "         0 /    488,204\n",
      "2018-08-11 |    292,826\n",
      "         0 /    292,826\n",
      "2018-08-10 |  1,021,266\n",
      "         0 /  1,021,266\n",
      " 1,000,000 /  1,021,266\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db_UTerm1200.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date (기간에 속하는 각 날짜) iteration\n",
    "for date in date_list:\n",
    "    inverted_idx = {}\n",
    "    data = []\n",
    "    \n",
    "    cur.execute(\"SELECT T.a_t_id, T.a_id, T.term \\\n",
    "                From Article A, Term T \\\n",
    "                WHERE A.a_id = T.a_id AND A.date = '{0}'\".format(date))\n",
    "    data.extend(cur.fetchall())\n",
    "\n",
    "    print('{0} | {1:10,}'.format(date, len(data)))\n",
    "\n",
    "    # Noun iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 500000) == 0:\n",
    "            print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "        a_t_id = d[0]\n",
    "        a_id = d[1]\n",
    "        noun = d[2]\n",
    "\n",
    "        if noun in inverted_idx.keys():\n",
    "            inverted_idx[noun].append((a_t_id, a_id))\n",
    "        else:\n",
    "            inverted_idx[noun] = []\n",
    "            inverted_idx[noun].append((a_t_id, a_id))\n",
    "\n",
    "    with open('db/inverted_index/inverted_index_' + date + '.pkl', 'wb') as f:\n",
    "        pickle.dump(inverted_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  94994 \n",
      "\n",
      "화재\n",
      "DF:  201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'da_20180823235953274'),\n",
       " (6263, 'da_20180823224951448'),\n",
       " (6881, 'da_20180823224523381'),\n",
       " (14097, 'da_20180823213907297'),\n",
       " (14835, 'da_20180823213513245'),\n",
       " (17481, 'da_20180823212259097'),\n",
       " (18377, 'da_20180823212032071'),\n",
       " (19937, 'da_20180823211324987'),\n",
       " (24802, 'da_20180823205613743'),\n",
       " (31011, 'da_20180823202638291')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug] Inverted index\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/inverted_index/inverted_index_{0}.pkl'.format(date), 'rb') as f:\n",
    "    inverted_idx = pickle.load(f)\n",
    "\n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index (섹션 + 날짜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "society | 2018-08-27 |    430,741\n",
      "society | 2018-08-26 |    173,525\n",
      "society | 2018-08-25 |    106,613\n",
      "society | 2018-08-24 |    355,193\n",
      "politics | 2018-08-27 |    139,342\n",
      "politics | 2018-08-26 |     98,742\n",
      "politics | 2018-08-25 |     95,142\n",
      "politics | 2018-08-24 |    124,337\n",
      "economic | 2018-08-27 |    400,002\n",
      "economic | 2018-08-26 |    168,854\n",
      "economic | 2018-08-25 |     57,087\n",
      "economic | 2018-08-24 |    262,503\n",
      "culture | 2018-08-27 |     77,487\n",
      "culture | 2018-08-26 |     37,853\n",
      "culture | 2018-08-25 |     27,879\n",
      "culture | 2018-08-24 |     98,166\n",
      "digital | 2018-08-27 |     68,862\n",
      "digital | 2018-08-26 |     32,453\n",
      "digital | 2018-08-25 |      8,146\n",
      "digital | 2018-08-24 |     45,884\n",
      "global | 2018-08-27 |     35,348\n",
      "global | 2018-08-26 |     53,108\n",
      "global | 2018-08-25 |     40,030\n",
      "global | 2018-08-24 |     73,461\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24']\n",
    "\n",
    "# Section interation\n",
    "for section in section_list:\n",
    "    # Date (기간에 속하는 각 날짜) iteration\n",
    "    for date in date_list:\n",
    "        inverted_idx = {}\n",
    "        data = []\n",
    "\n",
    "        cur.execute(\"SELECT T.a_t_id, T.a_id, T.term \\\n",
    "                    From Article A, Term T \\\n",
    "                    WHERE A.a_id = T.a_id AND A.section = '{0}' AND A.date = '{1}'\".format(section_dict[section], date))\n",
    "        data.extend(cur.fetchall())\n",
    "\n",
    "        print('{0} | {1} | {2:10,}'.format(section, date, len(data)))\n",
    "        \n",
    "        # Noun iteration\n",
    "        for data_idx, d in enumerate(data):\n",
    "#             if (data_idx % 500000) == 0:\n",
    "#                 print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "            a_t_id = d[0]\n",
    "            a_id = d[1]\n",
    "            noun = d[2]\n",
    "\n",
    "            if noun in inverted_idx.keys():\n",
    "                inverted_idx[noun].append((a_t_id, a_id))\n",
    "            else:\n",
    "                inverted_idx[noun] = []\n",
    "                inverted_idx[noun].append((a_t_id, a_id))\n",
    "\n",
    "        with open('db/inverted_index/inverted_index_' + section + '_' + date + '.pkl', 'wb') as f:\n",
    "            pickle.dump(inverted_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  50878 \n",
      "\n",
      "김선영\n",
      "DF:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(14689001, 'da_20180827235947759')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug] Inverted index\n",
    "section = 'society'\n",
    "date = '2018-08-27'\n",
    "\n",
    "with open('db/inverted_index/inverted_index_{0}_{1}.pkl'.format(section, date), 'rb') as f:\n",
    "    inverted_idx = pickle.load(f)\n",
    "\n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert TF-IDFs into Term Table\n",
    "---\n",
    "\n",
    "* 2018-08-10 - 2018-08-23은 고정된 2주 데이터의 TF-IDF로 고정\n",
    "\n",
    "\n",
    "* DB에는 최신 2주의 TF-IDF를 저장 (2018-08-14 - 2018-08-27)  \n",
    "\n",
    "\n",
    "※ UPDATE 쓰지 말자. 되도록이면 한 번에 INSERT. 특히 records 수가 많다면 WHERE 쓸 생각 절대 하지 말 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Article table 삭제\n",
    "# conn = sqlite3.connect('db/news_db_Term1200.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DROP TABLE Article\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380967\n",
      "381190\n",
      "380626\n",
      "380743\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24']\n",
    "\n",
    "for date in date_list:\n",
    "    with open('db/unique_nouns_idf/unique_nouns_idf_{0}_term.pkl'.format(date), 'rb') as f:\n",
    "        unique_nouns_idf = pickle.load(f)\n",
    "        \n",
    "    print(len(unique_nouns_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,000,000 /  1,151,782\n",
      "14,000,000 /    334,897\n",
      "13,000,000 /    959,544\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date iteration\n",
    "date_list = ['2018-08-27', '2018-08-26', '2018-08-25', '2018-08-24']\n",
    "for date in date_list:\n",
    "    with open('db/unique_nouns_idf/unique_nouns_idf_{0}_term.pkl'.format(date), 'rb') as f:\n",
    "        unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "    cur.execute(\"SELECT a_t_id, term, tf_article FROM Term WHERE date = '{0}'\".format(date))\n",
    "    data = cur.fetchall()\n",
    "\n",
    "    for data_idx, d in enumerate(data):       \n",
    "        if (data_idx % 100000) == 0:\n",
    "            print('{0:10,} / {1:10,}'.format(a_t_id - 1, len(data)))\n",
    "            \n",
    "        a_t_id = d[0]\n",
    "        noun = d[1]\n",
    "        tf = d[2]\n",
    "        idf = unique_nouns_idf[noun]\n",
    "\n",
    "    #     print(a_t_id, tf * idf)\n",
    "        try:\n",
    "            cur.execute(\"UPDATE Term \\\n",
    "                        SET tfidf = {0} \\\n",
    "                        WHERE a_t_id = {1}\"\n",
    "                        .format(tf * idf, a_t_id))\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('db/news_db_Term1200.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date iteration\n",
    "# for date in date_list:\n",
    "#     print(date)\n",
    "#     print('--------------------------------------------------\\n')\n",
    "    \n",
    "#     pkl_Term_i = {}\n",
    "\n",
    "#     cur.execute(\"SELECT a_id FROM Article \\\n",
    "#                  WHERE date = '{0}'\".format(date))\n",
    "#     a_ids = cur.fetchall()\n",
    "    \n",
    "#     for a_id_idx, a_id in enumerate(a_ids):\n",
    "#         if (a_id_idx % 1000) == 0:\n",
    "#             print('{0:10,} / {1:10,}'.format(a_id_idx, len(a_ids)))\n",
    "\n",
    "#         cur.execute(\"SELECT * \\\n",
    "#                     FROM Term \\\n",
    "#                     WHERE a_id = '{0}'\".format(a_id[0]))\n",
    "#         data = cur.fetchall()\n",
    "\n",
    "#         for d in data:\n",
    "#             a_t_id = d[0]\n",
    "#             cols = d[1:]\n",
    "\n",
    "#         pkl_Term_i[a_t_id] = cols\n",
    "                \n",
    "#     print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date iteration\n",
    "# for date in date_list[:7]:\n",
    "#     print(date)\n",
    "#     print('--------------------------------------------------\\n')\n",
    "\n",
    "#     unique_nouns_idf = {}\n",
    "#     with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date), 'rb') as f:\n",
    "#         unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "#     # TODO: inverted_idx에서 a_ids 가져오는 것으로 대체\n",
    "#     cur.execute(\"SELECT a_id FROM Article \\\n",
    "#                  WHERE date = '{0}'\".format(date))\n",
    "#     a_ids = cur.fetchall()\n",
    "\n",
    "#     for a_id_idx, a_id in enumerate(a_ids):\n",
    "#         if (a_id_idx % 1000) == 0:\n",
    "#             print('{0:10,} / {1:10,}'.format(a_id_idx, len(a_ids)))\n",
    "\n",
    "#         cur.execute(\"SELECT T.a_t_id, T.term, T.tf_article \\\n",
    "#                     FROM Term T \\\n",
    "#                     WHERE T.a_id = '{0}'\".format(a_id[0]))\n",
    "#         data = cur.fetchall()\n",
    "\n",
    "#         for d in data:\n",
    "#             a_t_id = d[0]\n",
    "#             noun = d[1]\n",
    "#             tf = d[2]\n",
    "#             idf = unique_nouns_idf[noun]\n",
    "\n",
    "# #             print(a_t_id, tf * idf)\n",
    "#             try:\n",
    "#                 cur.execute(\"UPDATE Term \\\n",
    "#                             SET tfidf = {0} \\\n",
    "#                             WHERE a_t_id = {1}\"\n",
    "#                             .format(tf * idf, a_t_id))\n",
    "#             except:\n",
    "#                 pass\n",
    "#         else:\n",
    "#             conn.commit()\n",
    "            \n",
    "#     print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Debug]\n",
    "# conn = sqlite3.connect('db/daum.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# unique_nouns_idf = {'태풍': 1., '솔릭': 100., '제주': 10000.}\n",
    "\n",
    "# for unique_noun_idx, item in enumerate(unique_nouns_idf.items()):\n",
    "#     if (unique_noun_idx % 10000) == 0:\n",
    "#         print('{0:10,} / {1:10,}'.format(unique_noun_idx, len(unique_nouns_idf)))\n",
    "    \n",
    "#     unique_noun = item[0]\n",
    "#     idf = item[1]\n",
    "#     print(unique_noun, idf)\n",
    "    \n",
    "#     cur.execute(\"SELECT a_t_id, tf_article FROM Term WHERE term = '{0}'\".format(unique_noun))\n",
    "\n",
    "#     data = cur.fetchall()\n",
    "    \n",
    "#     for d in data:\n",
    "#         a_t_id = d[0]\n",
    "#         tf = d[1]\n",
    "        \n",
    "#         cur.execute(\"UPDATE Term \\\n",
    "#                     SET tfidf = {0} \\\n",
    "#                     WHERE a_t_id = {1}\"\n",
    "#                     .format(tf * idf, a_t_id))\n",
    "#     else:\n",
    "#         conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기간(1주일)마다 pickle 저장\n",
    "* TODO: 일단 중지. 업데이트 방식 결정 후 필요하면 다시 코딩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23\n",
      "         0 /        944\n",
      "2018-08-22\n",
      "         0 /      1,082\n",
      "2018-08-21\n",
      "         0 /      1,170\n",
      "2018-08-20\n",
      "         0 /      1,267\n",
      "2018-08-19\n",
      "         0 /      1,213\n",
      "2018-08-18\n",
      "         0 /      1,198\n",
      "2018-08-17\n",
      "         0 /      1,336\n",
      "2018-08-16\n",
      "         0 /      1,330\n"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date (기간) iteration\n",
    "# for date_idx in range(len(date_list)-6):   \n",
    "#     unique_nouns_idf = {}\n",
    "#     with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date_list[date_idx]), 'rb') as f:\n",
    "#         unique_nouns_idf = pickle.load(f)\n",
    "#     pkl_Term_i = {}\n",
    "    \n",
    "#     print(date_list[date_idx])\n",
    "\n",
    "#     for unique_noun_idx, item in enumerate(unique_nouns_idf.items()):\n",
    "#         if (unique_noun_idx % 10000) == 0:\n",
    "#             print('{0:10,} / {1:10,}'.format(unique_noun_idx, len(unique_nouns_idf)))\n",
    "\n",
    "#         unique_noun = item[0]\n",
    "#         idf = item[1]\n",
    "\n",
    "#         cur.execute(\"SELECT T.a_t_id, T.a_id, T.term, T.tf_article \\\n",
    "#                     FROM Article A, Term T \\\n",
    "#                     WHERE A.a_id = T.a_id AND A.date = '{0}' AND T.term = '{1}'\".format(date, unique_noun))\n",
    "\n",
    "#         data = cur.fetchall()\n",
    "\n",
    "#         for d in data:\n",
    "#             a_t_id = d[0]\n",
    "#             a_id = d[1]\n",
    "#             noun = d[2]\n",
    "#             tf = d[3]\n",
    "#             tfidf = tf * idf\n",
    "\n",
    "#             pkl_Term_i[a_t_id] = (a_id, noun, tf, tfidf)\n",
    "                \n",
    "#     with open('db/Term/Term_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "#         pickle.dump(pkl_Term_i, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nouns:  54 \n",
      "\n",
      "a_t_id | [a_id, term, tf_article, tfidf]\n",
      "--------------------------------------------------\n",
      "1587 | ['da_20180815235603323', '현장', 0.55, 2.502891222417879]\n",
      "1769 | ['da_20180815235400307', '경찰', 0.5714285714285714, 2.2895104395067296]\n",
      "1510 | ['da_20180815235603323', '사건', 0.55, 2.668457720033069]\n",
      "1638 | ['da_20180815235510313', '사건', 0.7142857142857143, 3.465529506536453]\n",
      "1715 | ['da_20180815235400307', '처리', 0.5714285714285714, 2.772423605229162]\n",
      "1696 | ['da_20180815235400307', '조사', 1.0, 4.07359005876739]\n",
      "1656 | ['da_20180815235510313', '중인', 0.5714285714285714, 2.6004064648497445]\n",
      "1641 | ['da_20180815235510313', '수사', 0.6428571428571428, 2.81225574927731]\n",
      "1712 | ['da_20180815235400307', '수사', 0.5714285714285714, 2.499782888246498]\n",
      "1758 | ['da_20180815235400307', '건물', 0.5714285714285714, 2.772423605229162]\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Term_i TF-IDF\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/Term/Term_{0}.pkl'.format(date), 'rb') as f:\n",
    "    pkl_Term_i = pickle.load(f)\n",
    "    \n",
    "a_t_ids = tuple(pkl_Term_i.keys())[:10]\n",
    "\n",
    "print('The number of nouns: ', len(pkl_Term_i), '\\n')\n",
    "print('a_t_id | [a_id, term, tf_article, tfidf]')\n",
    "print('--------------------------------------------------')\n",
    "for a_t_id in a_t_ids:\n",
    "    print('{0} | {1}'.format(a_t_id, pkl_Term_i[a_t_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Unique Nouns and IDFs into U_Term Table\n",
    "---\n",
    "\n",
    "* DB에는 최신 2주의 TF-IDF를 저장 (2018-08-14 - 2018-08-27)  \n",
    "* 기존 데이터는 삭제됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "try:\n",
    "    cur.execute(\"DELETE FROM U_Term\")\n",
    "    cur.commit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 기간별 unique nouns\n",
    "date = '2018-08-27'\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}_term.pkl'.format(date), 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "# Insert data\n",
    "for unique_noun, idf in unique_nouns_idf.items():\n",
    "    record = (unique_noun, idf)\n",
    "    try:\n",
    "        cur.execute(\"INSERT INTO U_Term(u_term, idf) \\\n",
    "                    VALUES(?,?)\", record)\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = '2018-08-23'\n",
    "\n",
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # 전체 unique nouns\n",
    "# with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'rb') as f:\n",
    "#     unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "# # 기간별 unique nouns\n",
    "# with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date), 'rb') as f:\n",
    "#     unique_nouns_idf_clip = pickle.load(f)\n",
    "    \n",
    "# # Insert data\n",
    "# for unique_noun, idf in unique_nouns_idf.items():\n",
    "#     # 기간별 unique nouns에 존재하면 noun과 idf (2주치 IDF) 모두 insert\n",
    "#     if unique_noun in unique_nouns_idf_clip.keys():\n",
    "#         record = (unique_noun, idf)\n",
    "#         try:\n",
    "#             cur.execute(\"INSERT INTO U_Term(u_term, idf) \\\n",
    "#                         VALUES(?,?)\", record)\n",
    "#         except:\n",
    "#             pass\n",
    "# #     # 그렇지 않을 경우 noun만 insert\n",
    "# #     else:\n",
    "# #         try:\n",
    "# #             cur.execute(\"INSERT INTO U_Term(u_term) \\\n",
    "# #                         VALUES('{0}')\".format(unique_noun))\n",
    "# #         except:\n",
    "# #             pass\n",
    "# else:\n",
    "#     conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기간(1주)마다 pickle 저장 (23일까지 적용, 앞에서 이미 저장함.)\n",
    "   * Path: db/unique_nouns_idf/unique_nouns_idf_**[기준 날짜]**_term.pkl\n",
    "      * e.g. db/unique_nouns_idf/unique_nouns_idf_2018-08-23.pkl\n",
    " \n",
    " \n",
    "* 기간(2주)마다 pickle 저장 (24일부터 적용, 앞에서 이미 저장함.)\n",
    "   * Path: db/unique_nouns_idf/unique_nouns_idf_**[기준 날짜]**_term.pkl\n",
    "      * e.g. db/unique_nouns_idf/unique_nouns_idf_2018-08-27.pkl\n",
    "      \n",
    "      \n",
    "* 별도의 테이블에 전체(2주) 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('db/news_db_Term1200.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # 전체 unique nouns\n",
    "# with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'rb') as f:\n",
    "#     unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "# # Insert data\n",
    "# for unique_noun, idf in unique_nouns_idf.items():\n",
    "#     record = (unique_noun, idf)\n",
    "#     try:\n",
    "#         cur.execute(\"INSERT INTO U_Term(u_term, idf) \\\n",
    "#                     VALUES(?,?)\", record)\n",
    "#     except:\n",
    "#         pass\n",
    "# else:\n",
    "#     conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
