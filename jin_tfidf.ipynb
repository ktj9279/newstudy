{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from konlpy.tag import Kkma\n",
    "from math import log2, log10\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list = ['society', 'politics', 'economic', 'culture', 'digital', 'global']\n",
    "section_dict = {'society':'사회', 'politics':'정치', 'economic':'경제',\n",
    "               'culture':'문화', 'digital':'IT', 'global':'세계'}\n",
    "\n",
    "date_list = ['2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17',\n",
    "             '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 변수\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시로 데이터를 보관하는 변수. DB 또는 pickle로 저장함.\n",
    "# 여러 개의 기준이 있을 수 있음. 주석 참조.\n",
    "a_nouns_tf = {}          # Article\n",
    "a_noun_max_cnt = {}      # Article\n",
    "inverted_idx = {}        # 전체, 기간, section + 기간\n",
    "unique_nouns_idf = {}    # 전체, 기간, section + 기간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 형태 아님. 맨 앞에 section으로 된 key 빼야 함.\n",
    "\n",
    "\n",
    "**a_nouns_tf**  \n",
    "{'society': {'da_20180822201040252': {'인천': 0.1, '남동': 0.5, '남동공단': 0.8, ...}  \n",
    "...  \n",
    "}}\n",
    "\n",
    "\n",
    "**a_noun_max_cnt**  \n",
    "{'society': {'da_20180822201040252': 4, 'da_20180822201030249': 20, 'da_20180822200910228': 3, ...}  \n",
    "...  \n",
    "}\n",
    "\n",
    "\n",
    "**inverted_idx**  \n",
    "{'society': {'김영호': ['da_20180822194016575'],  \n",
    "'심상치가': ['da_20180822194312638'],  \n",
    "'최우수': ['da_20180822192938304', 'da_20180822191858060'], ...}  \n",
    "...  \n",
    "}\n",
    "\n",
    "\n",
    "**unique_nouns_idf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE Term(a_t_id      INTEGER PRIMARY KEY, \\\n",
    "                                   a_id        TEXT, \\\n",
    "                                   term        TEXT, \\\n",
    "                                   tf_article  REAL, \\\n",
    "                                   tfidf       REAL, \\\n",
    "                                   FOREIGN KEY (a_id) REFERENCES Article(a_id))\")\n",
    "    \n",
    "    conn.commit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE U_Term(u_term      TEXT PRIMARY KEY, \\\n",
    "                                     idf         REAL)\")\n",
    "    \n",
    "    conn.commit()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(\"DROP TABLE Term\")\n",
    "\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거\n",
    "---\n",
    "\n",
    "* by 한홀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NewsStopWord(word):\n",
    "    try:\n",
    "        int(word) #숫자일경우\n",
    "    except:        \n",
    "        if type(word) is str and word.__contains__('회차'):\n",
    "            return True\n",
    "        if len(word) == 1:\n",
    "            # 한글자 빠짐\n",
    "            return True\n",
    "        if re.search(r'\\d+',word) != None:\n",
    "            # 숫자가 하나라도 포함되면\n",
    "            return True\n",
    "               \n",
    "        newsDic = {\"기자\":1,\"배포\":1,\"금지\":1,\"뉴스\":1,\"저작권자\":1,\n",
    "                   \"기사\":1,\"전재\":1,\"무단\":1,\"무단전재\":1,\"구독\":1,\"기사보기\":1}\n",
    "        \n",
    "        pressDic = {'연합뉴스':1,'뉴시스':1,'뉴시스통신사':1,'통신사':1,\n",
    "                    '이데일리':1,'네이버':1,'다음':1,'시스':1,'뉴스1':1,'뉴스1코리':1}\n",
    "        \n",
    "        nothingDic = {'사진':1,'페이스북':1,'관련':1,'웹툰보기':1,'가기':1,'만큼':1,\n",
    "                     '최근':1,'재인':1,'올해':1,'시간':1,'판단':1,'추진':1,'우리':1,'반영':1,\n",
    "                      '상황':1,'호텔':1,'운영':1,'주요':1,'적극':1,'대상':1,'때문':1,\n",
    "                      '확인':1,'가능':1,'이야기':1,'규모':1,'개월':1,'종합':1,'위원회':1,\n",
    "                      '가운데':1,'분석':1,'다양':1,'문제':1,'기간':1,'마련':1,'지난해':1,'신청':1,'한편':1,'기준':1,\n",
    "                      '내용':1,'채널설정':1,'경우':1,'방안':1,'활용':1,'여러분':1,'기존':1,'최대':1,'스냅':1,'오전':1,'대비':1,\n",
    "                      '위원':1,'지난달':1,'이번달':1,'다음달':1,'위원장':1,'센터':1,'포함':1,'등에':1,'사진영상부':1,\n",
    "                      '구성':1,'수준':1,'기대':1,'공동':1,'안내':1,'활동':1,'첫날':1,'추가':1,'분야':1,'관리':1,\n",
    "                      '동안':1,'이용':1,'모습':1,'오늘':1,'논의':1,'입장':1,'업계':1,'내년':1,'블록':1,'체인':1,'실시간':1,\n",
    "                      '고객':1,'채널':1,'보기':1,'오후':1,'이번':1,'이날':1,'진행':1,'제공':1,'예정':1,'연합':1,'대표':1,\n",
    "                      '제보':1,'이상':1,'지원':1,'행사':1,'관계자':1,'설정':1,'계획':1,'단체':1,'타임':1,'이후':1,'발표':1\n",
    "                     }\n",
    "        if word in newsDic.keys() or word in pressDic.keys() or word in nothingDic.keys():\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Nouns and Compute Term Frequency (TF)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(obj):\n",
    "    try:\n",
    "        float(obj)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(content, k_ratio=0.5):\n",
    "    a_nouns_tf= {}\n",
    "    nouns_cnt = {}\n",
    "    a_noun_max_cnt = 0\n",
    "\n",
    "    # 연속된 공백 및 개행 제거\n",
    "    content = re.sub(r'[\\s]{2,}', ' ', content)\n",
    "    content = re.sub(r'[\\n]{2,}', '\\n', content)\n",
    "\n",
    "    # 문장 단위 토큰화\n",
    "    for sentence in nltk.sent_tokenize(content):\n",
    "        # 단어 단위 토큰화\n",
    "        for word in nltk.word_tokenize(sentence.strip()):\n",
    "            # 명사 추출\n",
    "            nouns = k.nouns(word)\n",
    "\n",
    "            for noun in nouns:\n",
    "                # 불용어 제거\n",
    "                if NewsStopWord(noun):\n",
    "                    continue\n",
    "\n",
    "                if noun in nouns_cnt.keys():\n",
    "                    nouns_cnt[noun] += 1\n",
    "                else:\n",
    "                    nouns_cnt[noun] = 1\n",
    "\n",
    "                if a_noun_max_cnt < nouns_cnt[noun]:\n",
    "                    a_noun_max_cnt = nouns_cnt[noun]\n",
    "\n",
    "    for noun in nouns_cnt.keys():\n",
    "        a_nouns_tf[noun] = k_ratio + (1 - k_ratio) * nouns_cnt[noun] / a_noun_max_cnt\n",
    "\n",
    "    return a_nouns_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_tf(a_id, content):\n",
    "#     a_nouns_tf[a_id] = {}\n",
    "#     nouns_cnt = {}\n",
    "#     a_noun_max_cnt[a_id] = 0\n",
    "\n",
    "#     # 연속된 공백 및 개행 제거\n",
    "#     content = re.sub(r'[\\s]{2,}', ' ', content)\n",
    "#     content = re.sub(r'[\\n]{2,}', '\\n', content)\n",
    "\n",
    "#     # 문장 단위 토큰화\n",
    "#     for idx, sentence in enumerate(nltk.sent_tokenize(content)):\n",
    "#         nouns_temp = []\n",
    "\n",
    "#         # 단어 단위 토큰화\n",
    "#         for word in nltk.word_tokenize(sentence.strip()):\n",
    "#             # 명사 추출\n",
    "#             nouns = k.nouns(word)\n",
    "\n",
    "#             for noun in nouns:\n",
    "#                 # 불용어 제거\n",
    "#                 if NewsStopWord(noun):\n",
    "#                     continue\n",
    "\n",
    "#                 if noun in nouns_cnt.keys():\n",
    "#                     nouns_cnt[noun] += 1\n",
    "#                 else:\n",
    "#                     nouns_cnt[noun] = 1\n",
    "\n",
    "#                 if a_noun_max_cnt[a_id] < nouns_cnt[noun]:\n",
    "#                     a_noun_max_cnt[a_id] = nouns_cnt[noun]\n",
    "\n",
    "# #             # 기본 불용어 제거\n",
    "# #             for stop_word in stop_words:\n",
    "# #                 nouns.remove(stop_word)\n",
    "\n",
    "# #             nouns_temp.extend(nouns)\n",
    "# #             nouns_temp = list(set(nouns_temp))\n",
    "\n",
    "# #         unique_nouns.extend(nouns_temp)\n",
    "# #         unique_nouns = list(set(unique_nouns))\n",
    "\n",
    "#     for noun in nouns_cnt.keys():\n",
    "#         a_nouns_tf[a_id][noun] = k_ratio + (1 - k_ratio) * nouns_cnt[noun] / a_noun_max_cnt[a_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debug] Extract Nouns and Compute Term Frequency (TF)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = \"\"\"제19호 태풍 ‘솔릭’이 제주도를 지나 북상하면서 광주 지역 모든 학교 학생들이 조기 하교 했다. 항공편은 모두 결항됐으며 무등산 입산이 통제됐다. \n",
    "\n",
    "#            23일 오후 광주 서구 하늘에 제19호 태풍 솔릭이 몰고온 먹구름이 가득하다.\n",
    "          \n",
    "\n",
    "# 23일 광주지방기상청에 따르면 제19호 태풍 ‘솔릭’은 이날 낮 12시 현재 제주 서귀포 서쪽 90km 부근 해상에서 시속 4km로 북진 하고 있다. 기상청은 태풍이 이날 오후 6시쯤 목포 서남쪽 80㎞ 해상을 지난뒤 오는 24일 새벽 전북 군산 인근으로 상륙할 것으로 보고 있다. \n",
    "# 솔릭이 예상보다 훨씬 느린 속도로 접근하면서 광주 시민들은 ‘조마조마’한 심정으로 피해 예방을 위해 총력을 다했다. 태풍의 영향으로 광주 지역에는 바람이 점차 강해지면서 이날 제주와 김포를 오가는 광주공항의 모든 항공편이 결항됐다. 무등산도 입산이 통제됐다. \n",
    "# 태풍이 접근하면서 광주시교육청은 전체 학교를 대상으로 ‘조기 하교’를 결정했다. 광주지역 유치원과 초·중·고는 오후 3시 이전에 조기 하교했다. 고등학교의 아간 자율학습도 금지됐다. 교육청은 학원에도 휴원을 적극 검토하고 하원 시간을 조정하도록 요청했다. \n",
    "# 광주·전남은 24일까지 100∼250㎜의 비가 내리고 해안과 지리산에는 400㎜ 넘게 내리는 곳도 있겠다. 광주·전남은 태풍의 중심에서 반경 25m 범위 안에 들어 바람도 강하게 불 것으로 기상청은 내다보고 있다.\"\"\"\n",
    "\n",
    "# a_nouns_tf = compute_tf(content)\n",
    "\n",
    "# a_nouns_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Nouns and TFs into Term Table\n",
    "---\n",
    "\n",
    "* DB에는 2018-08-23만 기록 (2018-08-17 - 2018-08-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date iteration (날짜별로 나눠서 처리)\n",
    "for date in date_list[:7]:\n",
    "    print(date)\n",
    "    print('--------------------------------------------------\\n')\n",
    "    \n",
    "    cur.execute(\"SELECT a_id, content FROM Article WHERE date = '{0}'\".format(date))\n",
    "    data = cur.fetchall()\n",
    "\n",
    "    # Content iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 5000) == 0:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM Term\")\n",
    "            t_size = cur.fetchone()[0]\n",
    "            \n",
    "            print('{0:6,} / {1:6,} | {2:13,}'.format(data_idx, len(data), t_size))\n",
    "            \n",
    "        # TF 계산\n",
    "        a_nouns_tf = compute_tf(content=d[1])\n",
    "        \n",
    "        # Insert data\n",
    "        for noun, tf in a_nouns_tf.items():\n",
    "            record = (d[0], noun, tf)\n",
    "            try:\n",
    "                cur.execute(\"INSERT INTO Term(a_id, term, tf_article) \\\n",
    "                            VALUES(?,?,?)\", record)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            conn.commit()\n",
    "\n",
    "    print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 에러 발생 또는 중간에 끊을 시 DB의 특정 데이터 삭제\n",
    "# # date = '2018-08-13'\n",
    "\n",
    "# for date in date_list[7:]:\n",
    "#     conn = sqlite3.connect('db/news_db.db')\n",
    "#     cur = conn.cursor()\n",
    "\n",
    "#     cur.execute(\"DELETE FROM Term \\\n",
    "#                 WHERE EXISTS (SELECT * \\\n",
    "#                               FROM Article \\\n",
    "#                               WHERE Article.date = '{0}' AND Article.a_id = Term.a_id )\".format(date))\n",
    "\n",
    "#     conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "너무 오래걸려서 다른 컴퓨터로 돌려서 pickle로 저장해놨다가 합침."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date iteration (날짜별로 나눠서 처리)\n",
    "# for date in date_list[-4:-3]:\n",
    "#     print(date)\n",
    "#     print('--------------------------------------------------\\n')\n",
    "    \n",
    "#     records = []\n",
    "    \n",
    "#     cur.execute(\"SELECT a_id, content FROM Article WHERE date = '{0}'\".format(date))\n",
    "#     data = cur.fetchall()\n",
    "\n",
    "#     # Content iteration\n",
    "#     for data_idx, d in enumerate(data):\n",
    "#         if (data_idx % 5000) == 0:\n",
    "#             cur.execute(\"SELECT COUNT(*) FROM Term\")\n",
    "#             t_size = cur.fetchone()[0]\n",
    "            \n",
    "#             print('{0:6,} / {1:6,} | {2:13,}'.format(data_idx, len(data), t_size))\n",
    "            \n",
    "#         # TF 계산\n",
    "#         a_nouns_tf = compute_tf(content=d[1])\n",
    "        \n",
    "#         # Insert data\n",
    "#         for noun, tf in a_nouns_tf.items():\n",
    "#             record = (d[0], noun, tf)\n",
    "#             records.append(record)\n",
    "# #             try:\n",
    "# #                 cur.execute(\"INSERT INTO Term(a_id, term, tf_article) \\\n",
    "# #                             VALUES(?,?,?)\", record)\n",
    "# #             except:\n",
    "# #                 pass\n",
    "# #         else:\n",
    "# #             conn.commit()\n",
    "\n",
    "#     with open('temp/records_{0}.pkl'.format(date), 'wb') as f:\n",
    "#         pickle.dump(records, f)\n",
    "\n",
    "#     print('\\n--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,256,343\n",
      "\n",
      "('da_20180813235953261', '충남', 0.75)\n",
      "('da_20180813235953261', '천안', 0.8125)\n",
      "('da_20180813235953261', '현금', 1.0)\n",
      "('da_20180813235953261', '수송', 0.875)\n",
      "('da_20180813235953261', '현금수송업체', 0.5625)\n",
      "('da_20180813235953261', '업체', 0.6875)\n",
      "('da_20180813235953261', '직원', 0.625)\n",
      "('da_20180813235953261', '보령', 0.625)\n",
      "('da_20180813235953261', '검거', 0.625)\n",
      "('da_20180813235953261', '천안서북경찰서', 0.6875)\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "date = '2018-08-13'\n",
    "\n",
    "with open('temp/records_{0}.pkl'.format(date), 'rb') as f:\n",
    "    records = pickle.load(f)\n",
    "\n",
    "print('{0:,}\\n'.format(len(records)))\n",
    "    \n",
    "for d in records[:10]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Insert pickled data into Term table\n",
    "# for date in date_list[-4:]:    # ['2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']\n",
    "#     with open('temp/records_{0}.pkl'.format(date), 'rb') as f:\n",
    "#         records = pickle.load(f)\n",
    "\n",
    "#     print('{0} | {1}'.format(date, len(records))\n",
    "#     print('--------------------------------------------------\\n')\n",
    "        \n",
    "#     for record in records:\n",
    "#         try:\n",
    "#             cur.execute(\"INSERT INTO Term(a_id, term, tf_article) \\\n",
    "#                         VALUES(?,?,?)\", record)\n",
    "#         except:\n",
    "#             pass\n",
    "#     else:\n",
    "#         conn.commit()\n",
    "            \n",
    "#     print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1200 Term 테이블을 사용해야 함. (현재 600짜리로 축소되어 있음.)**  \n",
    "**1200 Term 테이블을 pickle로 저장해서 사용할 것**\n",
    "\n",
    "## Extract Unique Nouns, Invert Index and\n",
    "## Compute Inverse Document Frequency (IDF)\n",
    "---\n",
    "\n",
    "* Inverted index and IDF (전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "inverted_idx = {}\n",
    "unique_nouns_idf = {}\n",
    "\n",
    "cur.execute(\"SELECT a_id, term FROM Term\")\n",
    "\n",
    "data = cur.fetchall()\n",
    "\n",
    "# Noun iteration\n",
    "for data_idx, d in enumerate(data):\n",
    "    if (data_idx % 1000000) == 0:\n",
    "        print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "    \n",
    "    noun = d[1]\n",
    "    a_id = d[0]\n",
    "    \n",
    "    if noun in inverted_idx.keys():\n",
    "        inverted_idx[noun].append(a_id)\n",
    "    else:\n",
    "        inverted_idx[noun] = []\n",
    "        inverted_idx[noun].append(a_id)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(*) FROM Article\")\n",
    "\n",
    "a_size = cur.fetchone()[0]\n",
    "        \n",
    "for noun, a_ids in inverted_idx.items():\n",
    "    df = len(a_ids)\n",
    "    unique_nouns_idf[noun] = log10(a_size / df)\n",
    "        \n",
    "with open('db/inverted_index/inverted_index.pkl', 'wb') as f:\n",
    "    pickle.dump(inverted_idx, f)\n",
    "    \n",
    "with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  382153 \n",
      "\n",
      "화재\n",
      "DF:  3866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['da_20180823235953274',\n",
       " 'da_20180823224951448',\n",
       " 'da_20180823224523381',\n",
       " 'da_20180823213907297',\n",
       " 'da_20180823213513245',\n",
       " 'da_20180823212259097',\n",
       " 'da_20180823212032071',\n",
       " 'da_20180823211324987',\n",
       " 'da_20180823205613743',\n",
       " 'da_20180823202638291']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug] Inverted index\n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of IDF unique nouns:  382153 \n",
      "\n",
      "화재 |  1.55\n",
      "화재원인 |  2.68\n",
      "원인 |  1.27\n",
      "어디 |   1.7\n",
      "인천 |  1.43\n",
      "윤태현 |   3.4\n",
      "태현 |  3.01\n",
      "인천시 |  2.13\n",
      "남동 |  2.16\n",
      "남동구 |  2.53\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index and IDF (기간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-08-23', '2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17']\n",
      "['2018-08-22', '2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16']\n",
      "['2018-08-21', '2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15']\n",
      "['2018-08-20', '2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14']\n",
      "['2018-08-19', '2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13']\n",
      "['2018-08-18', '2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12']\n",
      "['2018-08-17', '2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11']\n",
      "['2018-08-16', '2018-08-15', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-11', '2018-08-10']\n"
     ]
    }
   ],
   "source": [
    "# [Debug]\n",
    "for date_idx in range(len(date_list)-6):\n",
    "    print(date_list[date_idx:date_idx+7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 |  6,680,675 |     71,899\n",
      "         0 /  6,680,675\n",
      " 1,000,000 /  6,680,675\n",
      " 2,000,000 /  6,680,675\n",
      " 3,000,000 /  6,680,675\n",
      " 4,000,000 /  6,680,675\n",
      " 5,000,000 /  6,680,675\n",
      " 6,000,000 /  6,680,675\n",
      "2018-08-22 |  6,720,968 |     72,354\n",
      "         0 /  6,720,968\n",
      " 1,000,000 /  6,720,968\n",
      " 2,000,000 /  6,720,968\n",
      " 3,000,000 /  6,720,968\n",
      " 4,000,000 /  6,720,968\n",
      " 5,000,000 /  6,720,968\n",
      " 6,000,000 /  6,720,968\n",
      "2018-08-21 |  6,128,741 |     66,111\n",
      "         0 /  6,128,741\n",
      " 1,000,000 /  6,128,741\n",
      " 2,000,000 /  6,128,741\n",
      " 3,000,000 /  6,128,741\n",
      " 4,000,000 /  6,128,741\n",
      " 5,000,000 /  6,128,741\n",
      " 6,000,000 /  6,128,741\n",
      "2018-08-20 |  6,072,147 |     65,990\n",
      "         0 /  6,072,147\n",
      " 1,000,000 /  6,072,147\n",
      " 2,000,000 /  6,072,147\n",
      " 3,000,000 /  6,072,147\n",
      " 4,000,000 /  6,072,147\n",
      " 5,000,000 /  6,072,147\n",
      " 6,000,000 /  6,072,147\n",
      "2018-08-19 |  6,153,353 |     66,748\n",
      "         0 /  6,153,353\n",
      " 1,000,000 /  6,153,353\n",
      " 2,000,000 /  6,153,353\n",
      " 3,000,000 /  6,153,353\n",
      " 4,000,000 /  6,153,353\n",
      " 5,000,000 /  6,153,353\n",
      " 6,000,000 /  6,153,353\n",
      "2018-08-18 |  6,138,496 |     66,450\n",
      "         0 /  6,138,496\n",
      " 1,000,000 /  6,138,496\n",
      " 2,000,000 /  6,138,496\n",
      " 3,000,000 /  6,138,496\n",
      " 4,000,000 /  6,138,496\n",
      " 5,000,000 /  6,138,496\n",
      " 6,000,000 /  6,138,496\n",
      "2018-08-17 |  6,125,273 |     66,400\n",
      "         0 /  6,125,273\n",
      " 1,000,000 /  6,125,273\n",
      " 2,000,000 /  6,125,273\n",
      " 3,000,000 /  6,125,273\n",
      " 4,000,000 /  6,125,273\n",
      " 5,000,000 /  6,125,273\n",
      " 6,000,000 /  6,125,273\n",
      "2018-08-16 |  6,149,349 |     66,783\n",
      "         0 /  6,149,349\n",
      " 1,000,000 /  6,149,349\n",
      " 2,000,000 /  6,149,349\n",
      " 3,000,000 /  6,149,349\n",
      " 4,000,000 /  6,149,349\n",
      " 5,000,000 /  6,149,349\n",
      " 6,000,000 /  6,149,349\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date (기간) iteration\n",
    "for date_idx in range(len(date_list)-6):\n",
    "    inverted_idx = {}\n",
    "    unique_nouns_idf = {}\n",
    "    \n",
    "    data = []\n",
    "    a_size = 0\n",
    "    \n",
    "    # Date (기간에 속하는 각 날짜) iteration\n",
    "    for date in date_list[date_idx:date_idx+7]:\n",
    "        cur.execute(\"SELECT T.a_id, T.term \\\n",
    "                    From Article A, Term T \\\n",
    "                    WHERE A.a_id = T.a_id AND A.date = '{0}'\".format(date))\n",
    "\n",
    "        data.extend(cur.fetchall())\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM Article WHERE date = '{0}'\".format(date))\n",
    "\n",
    "        a_size += cur.fetchone()[0]\n",
    "\n",
    "    print('{0} | {1:10,} | {2:10,}'.format(date_list[date_idx], len(data), a_size))\n",
    "        \n",
    "    # Noun iteration\n",
    "    for data_idx, d in enumerate(data):\n",
    "        if (data_idx % 1000000) == 0:\n",
    "            print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "        noun = d[1]\n",
    "        a_id = d[0]\n",
    "\n",
    "        if noun in inverted_idx.keys():\n",
    "            inverted_idx[noun].append(a_id)\n",
    "        else:\n",
    "            inverted_idx[noun] = []\n",
    "            inverted_idx[noun].append(a_id)\n",
    "            \n",
    "    for noun, a_ids in inverted_idx.items():\n",
    "        df = len(a_ids)\n",
    "        unique_nouns_idf[noun] = log10(a_size / df)\n",
    "\n",
    "    with open('db/inverted_index/inverted_index_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "        pickle.dump(inverted_idx, f)\n",
    "        \n",
    "    with open('db/unique_nouns_idf/unique_nouns_idf_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "        pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  258680 \n",
      "\n",
      "화재\n",
      "DF:  1703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['da_20180823235953274',\n",
       " 'da_20180823224951448',\n",
       " 'da_20180823224523381',\n",
       " 'da_20180823213907297',\n",
       " 'da_20180823213513245',\n",
       " 'da_20180823212259097',\n",
       " 'da_20180823212032071',\n",
       " 'da_20180823211324987',\n",
       " 'da_20180823205613743',\n",
       " 'da_20180823202638291']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug] Inverted index\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/inverted_index/inverted_index_{0}.pkl'.format(date), 'rb') as f:\n",
    "    inverted_idx = pickle.load(f)\n",
    "\n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of IDF unique nouns:  258680 \n",
      "\n",
      "화재 |  1.63\n",
      "화재원인 |  2.71\n",
      "원인 |  1.27\n",
      "어디 |  1.71\n",
      "인천 |  1.39\n",
      "윤태현 |  3.39\n",
      "태현 |   3.0\n",
      "인천시 |  2.04\n",
      "남동 |  2.02\n",
      "남동구 |  2.32\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date), 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "    \n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted index and IDF (section + 기간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "society | 2018-08-23 |  2,461,835 |     28,067\n",
      "society | 2018-08-22 |  2,400,452 |     27,602\n",
      "society | 2018-08-21 |  2,174,006 |     25,113\n",
      "society | 2018-08-20 |  2,136,096 |     24,854\n",
      "society | 2018-08-19 |  2,156,734 |     25,098\n",
      "society | 2018-08-18 |  2,164,869 |     25,250\n",
      "society | 2018-08-17 |  2,149,636 |     25,200\n",
      "society | 2018-08-16 |  2,128,529 |     25,246\n",
      "politics | 2018-08-23 |    875,952 |      8,721\n",
      "politics | 2018-08-22 |    917,709 |      9,096\n",
      "politics | 2018-08-21 |    878,862 |      8,614\n",
      "politics | 2018-08-20 |    859,569 |      8,477\n",
      "politics | 2018-08-19 |    867,782 |      8,595\n",
      "politics | 2018-08-18 |    852,074 |      8,419\n",
      "politics | 2018-08-17 |    852,605 |      8,483\n",
      "politics | 2018-08-16 |    870,502 |      8,625\n",
      "economic | 2018-08-23 |  2,018,170 |     20,532\n",
      "economic | 2018-08-22 |  2,096,314 |     21,496\n",
      "economic | 2018-08-21 |  1,862,044 |     19,197\n",
      "economic | 2018-08-20 |  1,887,706 |     19,833\n",
      "economic | 2018-08-19 |  1,943,195 |     20,493\n",
      "economic | 2018-08-18 |  1,937,272 |     20,426\n",
      "economic | 2018-08-17 |  1,940,534 |     20,410\n",
      "economic | 2018-08-16 |  1,959,800 |     20,757\n",
      "culture | 2018-08-23 |    512,049 |      4,614\n",
      "culture | 2018-08-22 |    494,212 |      4,443\n",
      "culture | 2018-08-21 |    448,288 |      4,000\n",
      "culture | 2018-08-20 |    436,688 |      3,903\n",
      "culture | 2018-08-19 |    433,045 |      3,873\n",
      "culture | 2018-08-18 |    435,737 |      3,902\n",
      "culture | 2018-08-17 |    434,510 |      3,881\n",
      "culture | 2018-08-16 |    430,134 |      3,823\n",
      "digital | 2018-08-23 |    334,276 |      3,129\n",
      "digital | 2018-08-22 |    338,371 |      3,155\n",
      "digital | 2018-08-21 |    311,251 |      2,919\n",
      "digital | 2018-08-20 |    300,753 |      2,870\n",
      "digital | 2018-08-19 |    315,276 |      2,977\n",
      "digital | 2018-08-18 |    323,278 |      3,045\n",
      "digital | 2018-08-17 |    328,742 |      3,097\n",
      "digital | 2018-08-16 |    354,893 |      3,286\n",
      "global | 2018-08-23 |    478,393 |      6,836\n",
      "global | 2018-08-22 |    473,910 |      6,562\n",
      "global | 2018-08-21 |    454,290 |      6,268\n",
      "global | 2018-08-20 |    451,335 |      6,053\n",
      "global | 2018-08-19 |    437,321 |      5,712\n",
      "global | 2018-08-18 |    425,266 |      5,408\n",
      "global | 2018-08-17 |    419,246 |      5,329\n",
      "global | 2018-08-16 |    405,491 |      5,046\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Section interation\n",
    "for section in section_list:\n",
    "    # Date (기간) iteration\n",
    "    for date_idx in range(len(date_list)-6):\n",
    "        inverted_idx = {}\n",
    "        unique_nouns_idf = {}\n",
    "        \n",
    "        data = []\n",
    "        a_size = 0\n",
    "    \n",
    "        # Date (기간에 속하는 각 날짜) iteration\n",
    "        for date in date_list[date_idx:date_idx+7]:\n",
    "            cur.execute(\"SELECT T.a_id, T.term \\\n",
    "                        From Article A, Term T \\\n",
    "                        WHERE A.a_id = T.a_id AND A.section = '{0}' AND A.date = '{1}'\".format(section_dict[section], date))\n",
    "\n",
    "            data.extend(cur.fetchall())\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(*) FROM Article WHERE section = '{0}' AND date = '{1}'\".format(section_dict[section], date))\n",
    "\n",
    "            a_size += cur.fetchone()[0]\n",
    "\n",
    "        print('{0} | {1} | {2:10,} | {3:10,}'.format(section, date_list[date_idx], len(data), a_size))\n",
    "\n",
    "        # Noun iteration\n",
    "        for data_idx, d in enumerate(data):\n",
    "#             if (data_idx % 100000) == 0:\n",
    "#                 print('{0:10,} / {1:10,}'.format(data_idx, len(data)))\n",
    "\n",
    "            noun = d[1]\n",
    "            a_id = d[0]\n",
    "\n",
    "            if noun in inverted_idx.keys():\n",
    "                inverted_idx[noun].append(a_id)\n",
    "            else:\n",
    "                inverted_idx[noun] = []\n",
    "                inverted_idx[noun].append(a_id)\n",
    "                \n",
    "        for noun, a_ids in inverted_idx.items():\n",
    "            df = len(a_ids)\n",
    "            unique_nouns_idf[noun] = log10(a_size / df)\n",
    "\n",
    "        with open('db/inverted_index/inverted_index_' + section + '_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(inverted_idx, f)\n",
    "            \n",
    "        with open('db/unique_nouns_idf/unique_nouns_idf_' + section + '_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(unique_nouns_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique nouns:  143850 \n",
      "\n",
      "화재\n",
      "DF:  1089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['da_20180823235953274',\n",
       " 'da_20180823224951448',\n",
       " 'da_20180823224523381',\n",
       " 'da_20180823213907297',\n",
       " 'da_20180823213513245',\n",
       " 'da_20180823212259097',\n",
       " 'da_20180823212032071',\n",
       " 'da_20180823211324987',\n",
       " 'da_20180823205613743',\n",
       " 'da_20180823202638291']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Debug]\n",
    "section = 'society'\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/inverted_index/inverted_index_{0}_{1}.pkl'.format(section, date), 'rb') as f:\n",
    "    inverted_idx = pickle.load(f)\n",
    "    \n",
    "unique_noun = tuple(inverted_idx.keys())[0]\n",
    "\n",
    "print('The number of unique nouns: ', len(inverted_idx), '\\n')\n",
    "print(unique_noun)\n",
    "print('DF: ', len(inverted_idx[unique_noun]))\n",
    "inverted_idx[unique_noun][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of IDF unique nouns:  143850 \n",
      "\n",
      "화재 |  1.41\n",
      "화재원인 |  2.44\n",
      "원인 |  1.21\n",
      "어디 |  1.87\n",
      "인천 |  1.21\n",
      "윤태현 |  3.11\n",
      "태현 |  3.02\n",
      "인천시 |  1.72\n",
      "남동 |  1.74\n",
      "남동구 |  1.95\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Unique nouns IDF\n",
    "section = 'society'\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}_{1}.pkl'.format(section, date), 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "    \n",
    "unique_nouns = tuple(unique_nouns_idf.keys())[:10]\n",
    "\n",
    "print('The number of IDF unique nouns: ', len(unique_nouns_idf), '\\n')\n",
    "for unique_noun in unique_nouns:\n",
    "    print('{0} | {1:5.3}'.format(unique_noun, unique_nouns_idf[unique_noun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert TF-IDFs into Term Table\n",
    "---\n",
    "\n",
    "* 일단 별도의 테이블에 1,200만 단어 전부 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Article table 삭제\n",
    "# conn = sqlite3.connect('db/news_db_Term1200.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DROP TABLE Article\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0 /  6,680,675\n",
      " 1,000,000 /  6,680,675\n",
      " 2,000,000 /  6,680,675\n",
      " 3,000,000 /  6,680,675\n",
      " 4,000,000 /  6,680,675\n",
      " 5,000,000 /  6,680,675\n",
      " 6,000,000 /  6,680,675\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "cur.execute(\"SELECT a_t_id, term, tf_article FROM Term\")\n",
    "data = cur.fetchall()\n",
    "\n",
    "for d in data:       \n",
    "    a_t_id = d[0]\n",
    "    noun = d[1]\n",
    "    tf = d[2]\n",
    "    idf = unique_nouns_idf[noun]\n",
    "\n",
    "    if ((a_t_id - 1) % 1000000) == 0:\n",
    "        print('{0:10,} / {1:10,}'.format(a_t_id - 1, len(data)))\n",
    "\n",
    "#     print(a_t_id, tf * idf)\n",
    "    try:\n",
    "        cur.execute(\"UPDATE Term \\\n",
    "                    SET tfidf = {0} \\\n",
    "                    WHERE a_t_id = {1}\"\n",
    "                    .format(tf * idf, a_t_id))\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DB에는 2018-08-23만 기록 (2018-08-17 - 2018-08-23)\n",
    "* 2주치 IDF 사용\n",
    "\n",
    "\n",
    "※ UPDATE 쓰지 말자. 되도록이면 한 번에 INSERT. 특히 records 수가 많다면 WHERE 쓸 생각 절대 하지 말 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23\n",
      "--------------------------------------------------\n",
      "\n",
      "         0 /     13,471\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4adc9b0deeec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT *                     FROM Term                     WHERE a_id = '{0}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('db/news_db_Term1200.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Date iteration\n",
    "for date in date_list:\n",
    "    print(date)\n",
    "    print('--------------------------------------------------\\n')\n",
    "    \n",
    "    pkl_Term_i = {}\n",
    "\n",
    "    cur.execute(\"SELECT a_id FROM Article \\\n",
    "                 WHERE date = '{0}'\".format(date))\n",
    "    a_ids = cur.fetchall()\n",
    "    \n",
    "    for a_id_idx, a_id in enumerate(a_ids):\n",
    "        if (a_id_idx % 1000) == 0:\n",
    "            print('{0:10,} / {1:10,}'.format(a_id_idx, len(a_ids)))\n",
    "\n",
    "        cur.execute(\"SELECT * \\\n",
    "                    FROM Term \\\n",
    "                    WHERE a_id = '{0}'\".format(a_id[0]))\n",
    "        data = cur.fetchall()\n",
    "\n",
    "        for d in data:\n",
    "            a_t_id = d[0]\n",
    "            cols = d[1:]\n",
    "\n",
    "        pkl_Term_i[a_t_id] = cols\n",
    "                \n",
    "    print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date iteration\n",
    "# for date in date_list[:7]:\n",
    "#     print(date)\n",
    "#     print('--------------------------------------------------\\n')\n",
    "\n",
    "#     unique_nouns_idf = {}\n",
    "#     with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date), 'rb') as f:\n",
    "#         unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "#     # TODO: inverted_idx에서 a_ids 가져오는 것으로 대체\n",
    "#     cur.execute(\"SELECT a_id FROM Article \\\n",
    "#                  WHERE date = '{0}'\".format(date))\n",
    "#     a_ids = cur.fetchall()\n",
    "\n",
    "#     for a_id_idx, a_id in enumerate(a_ids):\n",
    "#         if (a_id_idx % 1000) == 0:\n",
    "#             print('{0:10,} / {1:10,}'.format(a_id_idx, len(a_ids)))\n",
    "\n",
    "#         cur.execute(\"SELECT T.a_t_id, T.term, T.tf_article \\\n",
    "#                     FROM Term T \\\n",
    "#                     WHERE T.a_id = '{0}'\".format(a_id[0]))\n",
    "#         data = cur.fetchall()\n",
    "\n",
    "#         for d in data:\n",
    "#             a_t_id = d[0]\n",
    "#             noun = d[1]\n",
    "#             tf = d[2]\n",
    "#             idf = unique_nouns_idf[noun]\n",
    "\n",
    "# #             print(a_t_id, tf * idf)\n",
    "#             try:\n",
    "#                 cur.execute(\"UPDATE Term \\\n",
    "#                             SET tfidf = {0} \\\n",
    "#                             WHERE a_t_id = {1}\"\n",
    "#                             .format(tf * idf, a_t_id))\n",
    "#             except:\n",
    "#                 pass\n",
    "#         else:\n",
    "#             conn.commit()\n",
    "            \n",
    "#     print('--------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Debug]\n",
    "# conn = sqlite3.connect('db/daum.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# unique_nouns_idf = {'태풍': 1., '솔릭': 100., '제주': 10000.}\n",
    "\n",
    "# for unique_noun_idx, item in enumerate(unique_nouns_idf.items()):\n",
    "#     if (unique_noun_idx % 10000) == 0:\n",
    "#         print('{0:10,} / {1:10,}'.format(unique_noun_idx, len(unique_nouns_idf)))\n",
    "    \n",
    "#     unique_noun = item[0]\n",
    "#     idf = item[1]\n",
    "#     print(unique_noun, idf)\n",
    "    \n",
    "#     cur.execute(\"SELECT a_t_id, tf_article FROM Term WHERE term = '{0}'\".format(unique_noun))\n",
    "\n",
    "#     data = cur.fetchall()\n",
    "    \n",
    "#     for d in data:\n",
    "#         a_t_id = d[0]\n",
    "#         tf = d[1]\n",
    "        \n",
    "#         cur.execute(\"UPDATE Term \\\n",
    "#                     SET tfidf = {0} \\\n",
    "#                     WHERE a_t_id = {1}\"\n",
    "#                     .format(tf * idf, a_t_id))\n",
    "#     else:\n",
    "#         conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기간(1주일)마다 pickle 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23\n",
      "         0 /        944\n",
      "2018-08-22\n",
      "         0 /      1,082\n",
      "2018-08-21\n",
      "         0 /      1,170\n",
      "2018-08-20\n",
      "         0 /      1,267\n",
      "2018-08-19\n",
      "         0 /      1,213\n",
      "2018-08-18\n",
      "         0 /      1,198\n",
      "2018-08-17\n",
      "         0 /      1,336\n",
      "2018-08-16\n",
      "         0 /      1,330\n"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('db/news_db.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Date (기간) iteration\n",
    "# for date_idx in range(len(date_list)-6):   \n",
    "#     unique_nouns_idf = {}\n",
    "#     with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date_list[date_idx]), 'rb') as f:\n",
    "#         unique_nouns_idf = pickle.load(f)\n",
    "#     pkl_Term_i = {}\n",
    "    \n",
    "#     print(date_list[date_idx])\n",
    "\n",
    "#     for unique_noun_idx, item in enumerate(unique_nouns_idf.items()):\n",
    "#         if (unique_noun_idx % 10000) == 0:\n",
    "#             print('{0:10,} / {1:10,}'.format(unique_noun_idx, len(unique_nouns_idf)))\n",
    "\n",
    "#         unique_noun = item[0]\n",
    "#         idf = item[1]\n",
    "\n",
    "#         cur.execute(\"SELECT T.a_t_id, T.a_id, T.term, T.tf_article \\\n",
    "#                     FROM Article A, Term T \\\n",
    "#                     WHERE A.a_id = T.a_id AND A.date = '{0}' AND T.term = '{1}'\".format(date, unique_noun))\n",
    "\n",
    "#         data = cur.fetchall()\n",
    "\n",
    "#         for d in data:\n",
    "#             a_t_id = d[0]\n",
    "#             a_id = d[1]\n",
    "#             noun = d[2]\n",
    "#             tf = d[3]\n",
    "#             tfidf = tf * idf\n",
    "\n",
    "#             pkl_Term_i[a_t_id] = (a_id, noun, tf, tfidf)\n",
    "                \n",
    "#     with open('db/Term/Term_' + date_list[date_idx] + '.pkl', 'wb') as f:\n",
    "#         pickle.dump(pkl_Term_i, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nouns:  54 \n",
      "\n",
      "a_t_id | [a_id, term, tf_article, tfidf]\n",
      "--------------------------------------------------\n",
      "1587 | ['da_20180815235603323', '현장', 0.55, 2.502891222417879]\n",
      "1769 | ['da_20180815235400307', '경찰', 0.5714285714285714, 2.2895104395067296]\n",
      "1510 | ['da_20180815235603323', '사건', 0.55, 2.668457720033069]\n",
      "1638 | ['da_20180815235510313', '사건', 0.7142857142857143, 3.465529506536453]\n",
      "1715 | ['da_20180815235400307', '처리', 0.5714285714285714, 2.772423605229162]\n",
      "1696 | ['da_20180815235400307', '조사', 1.0, 4.07359005876739]\n",
      "1656 | ['da_20180815235510313', '중인', 0.5714285714285714, 2.6004064648497445]\n",
      "1641 | ['da_20180815235510313', '수사', 0.6428571428571428, 2.81225574927731]\n",
      "1712 | ['da_20180815235400307', '수사', 0.5714285714285714, 2.499782888246498]\n",
      "1758 | ['da_20180815235400307', '건물', 0.5714285714285714, 2.772423605229162]\n"
     ]
    }
   ],
   "source": [
    "# [Debug] Term_i TF-IDF\n",
    "date = '2018-08-23'\n",
    "\n",
    "with open('db/Term/Term_{0}.pkl'.format(date), 'rb') as f:\n",
    "    pkl_Term_i = pickle.load(f)\n",
    "    \n",
    "a_t_ids = tuple(pkl_Term_i.keys())[:10]\n",
    "\n",
    "print('The number of nouns: ', len(pkl_Term_i), '\\n')\n",
    "print('a_t_id | [a_id, term, tf_article, tfidf]')\n",
    "print('--------------------------------------------------')\n",
    "for a_t_id in a_t_ids:\n",
    "    print('{0} | {1}'.format(a_t_id, pkl_Term_i[a_t_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Unique Nouns and IDFs into U_Term Table\n",
    "---\n",
    "\n",
    "* DB에는 2018-08-23만 기록 (2018-08-17 - 2018-08-23)\n",
    "* 2주치 IDF 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2018-08-23'\n",
    "\n",
    "conn = sqlite3.connect('db/news_db.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 전체 unique nouns\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf.pkl', 'rb') as f:\n",
    "    unique_nouns_idf = pickle.load(f)\n",
    "\n",
    "# 기간별 unique nouns\n",
    "with open('db/unique_nouns_idf/unique_nouns_idf_{0}.pkl'.format(date), 'rb') as f:\n",
    "    unique_nouns_idf_clip = pickle.load(f)\n",
    "    \n",
    "# Insert data\n",
    "for unique_noun, idf in unique_nouns_idf.items():\n",
    "    # 기간별 unique nouns에 존재하면 noun과 idf (2주치 IDF) 모두 insert\n",
    "    if unique_noun in unique_nouns_idf_clip.keys():\n",
    "        record = (unique_noun, idf)\n",
    "        try:\n",
    "            cur.execute(\"INSERT INTO U_Term(u_term, idf) \\\n",
    "                        VALUES(?,?)\", record)\n",
    "        except:\n",
    "            pass\n",
    "#     # 그렇지 않을 경우 noun만 insert\n",
    "#     else:\n",
    "#         try:\n",
    "#             cur.execute(\"INSERT INTO U_Term(u_term) \\\n",
    "#                         VALUES('{0}')\".format(unique_noun))\n",
    "#         except:\n",
    "#             pass\n",
    "else:\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기간(1주일)마다 pickle 저장 (앞에서 이미 저장함.)\n",
    "   * Path: db/unique_nouns_idf/unique_nouns_idf_**[기준 날짜]**.pkl\n",
    "      * e.g. db/unique_nouns_idf/unique_nouns_idf_2018-08-23.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
